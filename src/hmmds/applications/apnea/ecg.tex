% Notes on modeling ECGs with HMMs directly

\documentclass[12pt]{article}
\usepackage{graphicx,color}
\usepackage{amsmath, amsfonts}
\usepackage{placeins}

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\INTEGER}{\field{Z}}
\newcommand{\REAL}{\field{R}}
\newcommand{\COMPLEX}{\field{C}}
\newcommand{\id}{\mathbb{I}}
\newcommand{\variance}[1]{\field{V}\left[ #1 \right]}
\newcommand{\normal}[2]{{\cal N}\left(#1,#2 \right)}
\newcommand{\argmax}{\operatorname*{argmax}}

\title{HMMs for ECGs}
\author{Andrew M.\ Fraser}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:introduction}

I wrote explore.py to see how processing ECGs for apnea detection was
working.  Because I was disappointed by the performance of standard qrs
detectors that I got from
https://github.com/berndporr/py-ecg-detectors, I wrote my own by
building HMMs for ECG time series.

Recipes for making the models and figures described here appear in
Rules.mk.

\section{Outliers}
\label{sec:outliers}

While most of the ECG data consists of oscillations with amplitudes
less than $\pm$ 2mV, some intervals have spurious signals with $\pm$
10mV.  I call such signals \emph{lead noise}\footnote{The data in
  record \emph{C01} is less than .1mV till minute 5.1 when it goes to
  10 mV.  The usable data only starts at minute 10.24.}.  A model,
$\theta$, fit to the typical data can assign $P(y|\theta) = 0$ to lead
noise.  The observation models use inverse gamma priors for the
variance, and I use a single state, which I call the \emph{bad} state,
with prior that enforces a large variance to accommodate lead noise.
In particular the parameters are $\alpha=10^{2}$ and $\beta=10^{4}$
which in the reestimation formula
\begin{equation*}
  \sigma^2 = \frac{2 \beta + \text{data}}{2 \alpha +2 + \text{weight}}
\end{equation*}
gives $\sigma^2 \approx 100$ if there is not much data.

Since lead noise can start abruptly at any time, I allow transitions
to the bad state from every other state.  It's important to ensure
that those transitions are infrequent.  I set the probability of
transitions with the argument \emph{--noise\_parameters} of
model\_init.py to $10^{-165}$.

\section{A Model with a 49 State Chain}
\label{sec:mono20}

After trying many state topologies, I found that an HMM with a chain
of states that matches the ECG sequence PQRST with a fixed
duration\footnote{Each state in the chain has a single successor.}
works for record a01 if it is trained on that record.  Viterbi
decoding with the model after training for 50 iterations on record
\emph{a01} yields Figures~\ref{fig:dict_states_70}
and~\ref{fig:dict_states_71}.  Note that the segments in the two
figures have the same duration, 0.04 minutes or 2.4 seconds, and that
they are separated by 10.5 seconds.  The doubling of the patient's
heart rate reflects an apnea cycle.  The HMM tracks the changing heart
rate without missing or inserting beats.

\begin{figure}
  \centering
    \resizebox{0.7\textwidth}{!}{\includegraphics{dict_3_2_states_70.pdf}}
    \caption{Performance of the HMM with a single chain of 49 states
      trained to model PQRST sequences.  An short segment of the ECG
      signal appears in the upper plot, while the corresponding
      decoded state sequence appears in the lower plot.  The model was
      fit using 50 iterations of the Baum Welch algorithm.}
  \label{fig:dict_states_70}
\end{figure}

\begin{figure}
  \centering
  \resizebox{0.7\textwidth}{!}{\includegraphics{dict_3_2_states_71.pdf}}
  \caption{Performance of the same model used for
    Fig.~\ref{fig:dict_states_70}, but looking at a different
    segment of the data.}
  \label{fig:dict_states_71}
\end{figure}

\section{A Successful Model}
\label{sec:awkward}

On 2023-03-31 I created a model that I trained on only the a01 record.
It worked surprisingly well on all of the other CINC-2000 records
except a19.  Here is a list of features of the code and model that I
think are important:
\begin{description}
\item[Joint Observation Model] My old code for training created
  special classes that I called \emph{bundles}.  I eliminated bundles
  and wrote a more general class called \emph{JointObservation} that
  can combine any number of component observation models of any type.
  I also wrote an observation class called \emph{ClassObservation}
  designed to be used as a component of a JointObservation.  A
  ClassObservation instance deterministically produces an observation
  that indicates the class of which the state is a member.  The
  reestimate method of ClassObservation does nothing.  Its parameters
  never change.
\item[Modeling Outliers] In a ClassObservation instance the sets of
  states defined by the classes constitute a disjoint and exhaustive
  partitioning.  To accommodate outliers in ECG data, I wrote a
  subclass of ClassObservation called \emph{BadObservation} that
  supports states that have positive likelihood for all classes.  I
  also wrote a subclass of HMM for which training does not modify the
  state transition probabilities.  Using that subclass, I set the
  probability of each other state transitioning to the single state
  that models outliers to be extremely small.
\item[Plausible Priors] Before I wrote the classes to model outliers
  specifically, I used priors that forced large variances for states
  that modeled outliers.  With the new classes, the priors give a
  variance of 100 for the outliers which is appropriate because the
  signal is clamped at $\pm 10$ mV.
\end{description}

\begin{figure}
  \centering
  \resizebox{1.2\textwidth}{!}{\includegraphics{a01a19c02.pdf}}
  \caption{Subsets of data from three CINC-2000 records.  The trace
    labeled \emph{c02 states} illustrates the performance of the model
    trained on a01 alone.  I am surprised that the same model fails on
    the ECG from a19 because that ECG seems more similar to a01 than
    c02.}
  \label{fig:a01a19c02}
\end{figure}

\begin{figure}
  \centering
  \resizebox{1.0\textwidth}{!}{\includegraphics{train_log.pdf}}
  \caption{Iterative training progress.  Each trace is the log of a
    quantity devided by the number of training samples.}
  \label{fig:train_log.pdf}
\end{figure}

\begin{figure}
  \centering
  \resizebox{1.0\textwidth}{!}{\includegraphics{like_a14_x07.pdf}}
  \caption{Plots of $\log\left(p(y[t]|y[:t]) \right)$ for records a14
    and x07.  Those records have the high and low likelihoods in
    Table~\ref{tab:cross_entropy}.}
  \label{fig:train_log.pdf}
\end{figure}

\begin{table}
  \centering
    \input{table.tex}
  \caption[Cross Entropy]{Cross entropy and fraction of each record
    that is plausible for a model trained on record a01.}
  \label{tab:cross_entropy}
\end{table}

\section{Quadratic Fit to Peak}
\label{sec:quadratic}

Here I find an estimate of the location of a peak based data sampled
periodically.  I fit a quadratic function to a local maximum and its
two neighbors.  WLOG the (x,y) sample pairs are $[(-d,a), (0,b),
(d,c)]$, and the quadratic function is
\begin{equation*}
  y(x) = \alpha x^2 + \beta x + \gamma.
\end{equation*}
Some manipulation yields:
\begin{align*}
  \alpha &= \frac{a+c-2b}{2d^2} \\
  \beta &= \frac{c-a}{2d} \\
  \gamma &= b \\
  \frac{d}{dx} y(x) &= 2x\alpha + \beta
\end{align*}
So the quadratic estimate of the peak location is
\begin{equation*}
  \argmax_x y(x) = \frac{-\beta}{2\alpha} = \frac{d(a-c)}{2(a+c-2b)}.
\end{equation*}

\section{To Do}
\label{sec:todo}

\begin{itemize}
\item Plot training curve
\item Create table with scores (cross entropy and \% noise) for each
  record
\item Plot conditional likelihood vs t
\item Plot simulated data
\end{itemize}
\section{Times}
\label{sec:times}


\end{document}
