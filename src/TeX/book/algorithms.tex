\chapter{Basic Algorithms}
\label{chap:algorithms}

In Section~\ref{sec:intro_hmm} we mentioned the Viterbi algorithm for
finding the state sequence that has the highest probability given an
observation sequence, the forward algorithm for calculating the
probability of an observation sequence, and the Baum-Welch algorithm
for finding a parameter vector that at least locally maximizes the
likelihood given an observation sequence.  This chapter explains the
details of those algorithms.  Much of the literature on the algorithms
and much of the available computer code uses Ferguson's
\cite{Ferguson80} notation.  In particular, Rabiner's \cite{Rabiner89}
widely cited article follows Ferguson's notation.  Let us begin by
establishing our notation for the model parameters.
\setlength{\nomlabelwidth}{2.7cm}%
\begin{symbdescription}
\item[$\bm{P_{\ti{S}{t+1}\given \ti{S}{t}}(s\given {\tilde s})}$] The probability
  that at time $t+1$ the system will be in state $s$ given that at
  time $t$ it was in state ${\tilde s}$.  Notice that the parameter is
  independent of time $t$.  Ferguson called these parameters \emph{the
    $A$ matrix} with
  $a_{i,j} = P_{\ti{S}{t+1}\given \ti{S}{t}}(s_j\given s_i)$.
\item[$\bm{P_{\ti{S}{1}}(s)}$] The probability that the system will
  start in state $s$ at time 1.  Ferguson used $a$ to represent this
  vector of probabilities with $a_i = P_{\ti{S}{1}} \left(s_i \right)$.
\item[$\bm{P_{\ti{Y}{t}\given \ti{S}{t}}(y\given s)}$] The probability
  that the observation value is $y$ given that the system is in
  state $s$.  Ferguson used $b$ to represent this with
  $b_j(k) = P_{\ti{Y}{t}\given \ti{S}{t}} \left(y_k\given s_j \right)$.
\item[$\bm{\parameters}$] The entire collection of parameters.  For
  example, iteration of the Baum-Welch algorithm produces a sequence
  of parameter vectors $\ts{\parameters}{0}{N}$ with
  $P\left(\ts{y}{0}{T}\given \ti{\parameters}{n+1}\right) \geq
  P\left(\ts{y}{0}{T}\given \ti{\parameters}{n}\right)\, : 1 < n < N$.
  Instead of $\parameters$, Ferguson used $\lambda$ to denote the
  entire collection of parameters.
  \nomenclature[gh]{$\bm{\parameters}$}{The entire collection of
    parameters that defines a model.}
\end{symbdescription}

\section{The Forward Algorithm}
\label{sec:forward}
\index{forward algorithm|textbf}

Given $\parameters$, the forward algorithm calculates
$P(\ts{y}{0}{T}\given \parameters)$, and several useful intermediate terms.
Figure~\ref{fig:forward} sketches the basic recursion's structure.
Since we are considering only one set of parameters, we will drop the
dependence of probabilities on $\parameters$ from the notation for the
remainder of this section.  In the example of Eqn.~\eqref{eq:pcalc} we
found that we could write the right hand terms of
\begin{equation*}
   P\left( \ts{y}{0}{T} \right) = \sum_{\ts{s}{0}{T}}
   P\left( \ts{y}{0}{T},\ts{s}{0}{T} \right)
\end{equation*}
easily from the model assumptions.  Unfortunately, the number of
possible sequences $\ts{s}{0}{T}$ is exponential in $T$, and it is
not feasible to do the sum for even modest lengths $T$.

\begin{sidewaysfigure}[htbp]
  %% The following definitions are invoked by the XFig forward.fig.
  \centering{\plotsize%
    %% Column a
    \def\colaa{$\alpha(t-1,s_1)$}%
    \def\colab{$\alpha(t-1,s_2)$}%
    \def\colac{$\alpha(t-1,s_3)$}%
    %% Column b
    \def\colba{$P \left(s_1\given \ts{y}{0}{t} \right)$}%
    \def\colbb{$P \left(s_2\given \ts{y}{0}{t} \right)$}%
    \def\colbc{$P \left(s_3\given \ts{y}{0}{t} \right)$}%
    \def\sumeqforwardAthree{%
      \begin{minipage}[t]{1.8in}
        \raggedright%
        Weighted\\%
        sum of prior\\%
        $\alpha$'s\\%
        Eqn.~\eqref{eq:forwardA3}
      \end{minipage}}%
    %% Column c
    \def\colca{$P \left(s_1,\ti{y}{t}\given \ts{y}{0}{t} \right)$}%
    \def\colcb{$P \left(s_2,\ti{y}{t}\given \ts{y}{0}{t} \right)$}%
    \def\colcc{$P \left(s_3,\ti{y}{t}\given \ts{y}{0}{t} \right)$}%
    \def\prdeqforwardBtwo{%
      \begin{minipage}[t]{2.3in}
        \raggedright%
        Multiply\\%
        observation\\%
        probability\\%
        Eqn.~\eqref{eq:forwardB2}
      \end{minipage}}%
    %% Column d
    \def\coldb{$P(\ti{y}{t}\given \ts{y}{0}{t})$}%
    \def\prdeqforwardC{%
      \begin{minipage}[t]{1.8in}
        \raggedright%
        Add, to get\\%
        $\gamma(t)$, $P \left(\ti{y}{t}\given \ts{y}{0}{t} \right)$\\%
        Eqn.~\eqref{eq:forwardC}
      \end{minipage}}%
    %% Column e
    \def\colea{$\alpha(t,s_1)$}%
    \def\coleb{$\alpha(t,s_2)$}%
    \def\colec{$\alpha(t,s_3)$}%
    \def\quoteqforwardD{%
      \begin{minipage}[t]{1.8in}
        \raggedright% 
        Normalize\\%
        new $\alpha$'s\\%
        Eqn.~\eqref{eq:forwardD}
      \end{minipage}}%
    \input{forward.pdf_t}}
  \vspace{5 em}
  \caption[\comment{fig:forward }Dependency relations in the forward algorithm.]%
  {Dependency relations in the forward algorithm (See
    Eqns.~\eqref{eq:forwardA}-\eqref{eq:forwardD} in the text).}
  \label{fig:forward}
\end{sidewaysfigure}
\afterpage{\clearpage}%% Print this right here please.

The forward algorithm regroups the terms and produces the desired
result using order $T$ calculations.  For each time $t:\, 1 < t \leq
T$ we calculate $P(\ti{y}{t}\given \ts{y}{0}{t})$, and in principle we can
write
\begin{equation*}
  P(\ts{y}{0}{T}) = P(\ti{y}{0}) \prod_{t=1}^{T-1} P(\ti{y}{t}\given \ts{y}{0}{t}).
\end{equation*}
However the values of $P(\ti{y}{t}\given \ts{y}{0}{t})$ are typically
small compared to 1, and the product of many such terms is too small
to be represented even in double precision.  Working with logarithms
avoids underflow:

\begin{equation}
  \mlabel{eq:logP}
  \log\left( P(\ts{y}{0}{T}) \right) = \log\left( P(\ti{y}{0})
 \right) + \sum_{t=1}^T \log\left( P(\ti{y}{t}\given \ts{y}{0}{t})\right).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
%%% #1 = time #2 = state
\newcommand{\alphax}[4]{% updated state distribution
  \colorbox{yellow}{%
    $P_{\ti{#1}{#2}\given \ts{Y}{0}{#3}}\left(#4 \given \ts{y}{0}{#3} \right)$%
  }%
}%
%%%
%%%
\newcommand{\StateForecast}[3]{% state forecast
  \colorbox{green}{%
    $P_{\ti{#1}{#3}\given \ts{Y}{0}{#3}} \left(#2 \given  \ts{y}{0}{#3} \right)$%
  }%
}%
%%%
\newcommand{\gammax}{%
  \colorbox{cyan}{%
    $P(\ti{y}{t}\given \ts{y}{0}{t})$%
  }%
}%
%%%
%%%
\newcommand{\JointForecast}[3]{% joint forecast
  \colorbox{pink}{%
    $P_{\ti{#1}{#3},\ti{Y}{#3}\given \ts{Y}{0}{#3}} \left(#2,\ti{y}{#3}\given \ts{y}{0}{#3}\right)$%
  }%
}%
%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For each time step we do the four calculations specified in the
equations below\footnote{In \eqref{code:forward} on
  page~\pageref{code:forward} in Section~\ref{sec:Clarity} we present
  four lines of python code that implement these four calculations.}.
In these equations we use color highlighting to emphasize repeated
instances of the same quantity.  The algorithm saves the following
intermediate results
%
\nomenclature[ga]{$\alpha(t,s)$}{The conditional probability that at time
  $t$ the system is in state $s$ given the observations up to the
  present,
  \begin{equation*}
    \alpha(t,s) \equiv  P_{S(t) \given \ts{Y}{0}{t}}\left(s \given \ts{y}{0}{t}
    \right).
  \end{equation*}
  Also called the forward updated distribution in the Kalman filter
  literature.}
%
\nomenclature[gc]{$\gamma(t)$}{The probability of the present observation
  given the history.}%, $P(y(t)\given y_1^{t-1}$.}
\begin{subequations}
  \label{eq:ForwardDefinitions}
  \begin{align}
    \label{eq:alpha}
    \alpha(t,s) &\equiv \alphax{S}{t}{t+1}{s} && \text{The updated state distribution}\\
    \label{eq:gamma}
    \ti{\gamma}{t} &\equiv \gammax  && \text{The incremental likelihood}.
  \end{align}
  In words\footnote{In Ferguson's notation $\alpha(t,s) \equiv
    P_{\ti{S}{t},\ts{Y}{0}{t}} \left(s,\ts{y}{0}{t} \right)$.  Our
    notation differs from that by a factor of $P(\ts{y}{0}{t})$.
    Ferguson did not have notation for the term
    $P(\ti{y}{t}\given \ts{y}{0}{t})$ which we call $\gamma$, but he did use
    $\gamma$ for quantities that we will denote by $w$ in
    Eqns.~\eqref{eq:wit} and \eqref{eq:wijt}.}, $\alpha(t,s)$ is the
  conditional probability of being in state $s$ at time $t$ given all
  of the past observations including $\ti{y}{t}$ and $\ti{\gamma}{t}$ is the
  conditional probability of the observation at time $t$ given all of
  the previous observations.  We also define two intermediate terms that
  are not saved
  \begin{align}
    \label{eq:StateForecast}
    a(s,t) &\equiv \StateForecast{S}{s}{t}  && \text{The state forecast} \\
    \label{eq:JointForecast}
    \tilde a(s,t) &\equiv \JointForecast{S}{s}{t}  && \text{The joint forecast}.
  \end{align}
\end{subequations}

To initialize the algorithm, one assigns
\begin{align*}
  \ti{\gamma}{0} &= P_{\ti{Y}{0}}(\ti{y}{0}) =
              \sum_{s}P_{\ti{S}{0}}(s)P_{\ti{Y}{t}\given \ti{S}{t}}(\ti{y}{0}\given s)
  \\
  \alpha(0,s) &= P_{\ti{S}{0}\given \ti{Y}{0}}(s\given \ti{y}{0}) =
  \frac{P_{\ti{S}{0}}(s)P_{\ti{Y}{t}\given \ti{S}{t}}(\ti{y}{0}\given s)}
  {\ti{\gamma}{0}}
~\forall s \in \states
\end{align*}
where the distributions $P_{\ti{S}{0}}$ and $P_{\ti{Y}{t}\given \ti{S}{t}}$
are model parameters.  After initialization, the algorithm iterates in
a loop doing the following four calculations: 
\begin{description}
\item[Forecast the Distribution of States] Find for time $t$ and each
  possible state $s$ the conditional probability, given the
  previous observations, of the state being $s$ at time $t$:
  \begin{subequations}
    \label{eq:forwardA}
    \begin{align}
      \mlabel{eq:forwardA1}
      \StateForecast{S}{s}{t} & = \sum_{{\tilde s}\in\states}
      P_{\ti{S}{t},\ti{S}{t-1}\given \ts{Y}{0}{t}} \left( s, {\tilde s}
      \given \ts{y}{0}{t} \right)\\
      \mlabel{eq:forwardA2}
      & = \sum_{{\tilde s}\in\states} \left(
        P_{\ti{S}{t}\given \ti{S}{t-1},\ts{Y}{0}{t}}
        \left( s\given {\tilde s},\ts{y}{0}{t} \right) \right. \nonumber \\ &
      \left. \times \alphax{S}{t-1}{t}{{\tilde s}} \right) \\
      \mlabel{eq:forwardA3}
      & = \sum_{{\tilde s}\in\states} P_{\ti{S}{t}\given \ti{S}{t-1}} \left( s\given {\tilde s} \right)
      \cdot \alpha(t-1,{\tilde s}) .
    \end{align}
  \end{subequations}
  We justify the operations as follows:
  \begin{description}
  \item[\eqref{eq:forwardA1}] $P(a) = \sum_b P(a,b)$
  \item[\eqref{eq:forwardA2}] $P(a\given b)\cdot P(b) = P(a,b)$
  \item[\eqref{eq:forwardA3}] Assumption of
    Eqn.~\eqref{eq:assume_markov} on page~\pageref{eq:assume_markov}: The state process is Markov
  \end{description}
\item[Forecast the Joint Probability of States and the Current
  Observation] Find for time $t$ and each possible state $s$, the
  conditional probability, given the previous observations, of the
  state being $s$ and the observation being $\ti{y}{t}$ (the value
  actually observed):
  \begin{subequations}
    \label{eq:forwardB}
    \begin{align}
      \mlabel{eq:forwardB1}
      \JointForecast{S}{s}{t} &= P_{\ti{Y}{t} \given 
        \ti{S}{t},\ts{Y}{0}{t}}\left(\ti{y}{t} \given s, \ts{y}{0}{t}
      \right) \nonumber \\& \times \StateForecast{S}{s}{t}\\
      \mlabel{eq:forwardB2}
      &= P_{\ti{Y}{t} \given  \ti{S}{t}}\left(\ti{y}{t} \given s \right) \cdot
      \StateForecast{S}{s}{t} .
    \end{align}
  \end{subequations}
We justify the equations as follows:
\begin{description}
\item[\eqref{eq:forwardB1}]  $P(a\given b)\cdot P(b) = P(a,b)$
\item[\eqref{eq:forwardB2}] Assumption of
  Eqn.~\eqref{eq:assume_output} on page~\pageref{eq:assume_output}:
  The observations are conditionally independent given the states
\end{description}
\item[Calculate the Conditional Probability of the Current Observation]
  Find for time $t$, the conditional probability, given the previous
  observations, of the observation being $\ti{y}{t}$ (the observation actually
  observed):
  \begin{align}
    \mlabel{eq:forwardC}
    \gammax &= \sum_{s \in \states} \JointForecast{S}{s}{t} .
  \end{align}
Equation~\eqref{eq:forwardC} is an application of $P(a) = \sum_b P(a,b)$.
\item[Calculate the Updated Distribution of States] For each possible
  state $s$, find the conditional probability of being in that state
  at time $t$ given all of the observations up to time $t$.  Note that
  this differs from the first calculation, \eqref{eq:forwardA}, in that
  the conditioning event includes $\ti{y}{t}$:%
  \begin{subequations}
    \label{eq:forwardD}
    \begin{align}
      \mlabel{eq:forwardD1}
      \alpha(t,s) &\equiv
      \alphax{S}{t}{t+1}{s} \\
      \mlabel{eq:forwardD2}
      &= \JointForecast{S}{s}{t} \div \gammax
    \end{align}
  \end{subequations}
  Equation \eqref{eq:forwardD2} is an application of Bayes rule, \ie,
  $P(a\given b)\cdot P(b) = P(a,b)$.
\end{description}
Note that given the incremental likelihoods $\ti{\gamma}{t}$ the
forward algorithm simply alternates between multiplying a state
distribution by a likelihood and multiplying the result by the matrix
of state transition probabilities, \ie,
\begin{subequations}
    \label{eq:2LineForward}
  \begin{align}
    \label{eq:alpha2a}
    a(t,s) &= \sum_{\tilde s} P_{\ti{s}{1}\given\ti{s}{0}} \left(s\given
             \tilde s \right) \alpha(t-1,\tilde s) \\
    \label{eq:a2alpha}
    \alpha(t,s) &= \frac{P(\ti{y}{t}|s) a(t,s)}{\ti{\gamma}{t}}.
  \end{align}
\end{subequations}


\section{The Viterbi Algorithm}
\label{sec:viterbi}

For some applications, one must estimate the sequence of states based
on a sequence of observations.  The Viterbi algorithm %
\index{Viterbi algorithm|textbf}%
finds the \emph{best} sequence $\ts{\hat s}{1}{T}$ in the sense of
maximizing the probability $P\left( \ts{s}{0}{T}\given \ts{y}{0}{T}
\right)$.  That is equivalent to maximizing $\log \left( P\left(
    \ts{y}{0}{T},\ts{s}{0}{T} \right) \right)$ because $P \left(
  \ts{y}{0}{T} \right)$ is simply a constant, and the $\log$ is
monotonic, ie,
\begin{align*}
  \ts{\hat s}{0}{T} &\equiv \argmax_{\ts{s}{0}{T}}
  P(\ts{s}{0}{T}\given \ts{y}{0}{T})\\
  &= \argmax_{\ts{s}{0}{T}} \left( P(\ts{s}{0}{T}\given \ts{y}{0}{T}) \cdot
    P(\ts{y}{0}{T}) \right)\\
  &= \argmax_{\ts{s}{0}{T}} \log \left( P(\ts{y}{0}{T},\ts{s}{0}{T})
  \right).
\end{align*}
As in the implementation of the forward algorithm, we use logs to
avoid numerical underflow.  If we define $\log \left( P\left(
    \ts{y}{0}{t}, \ts{s}{0}{t} \right) \right)$ as the
\emph{utility} (negative cost) of the state sequence $\ts{s}{0}{t}$
given the observation sequence $\ts{y}{0}{t}$, then the Viterbi
algorithm finds the maximum utility state sequence.

Initially one calculates
$\log\left(P_{\ti{Y}{0},\ti{S}{0}} \left(\ti{y}{0}, s \right) \right)$
for each state $s \in \states$.  Then for each successive time step
$t: 1 \leq t < T$ one considers each state and determines the best
predecessor for that state and the utility of the best state sequence
ending in that state.  The Viterbi algorithm and the forward algorithm
have similar structures (see Fig.~\ref{fig:forward} and
Fig.~\ref{fig:viterbiB}).  Roughly, the forward algorithm does a sum
over predecessor states, while the Viterbi algorithm finds a maximum
over predecessor states. We use the following notation and equations
to describe and justify the algorithm.

\subsubsection*{Notation:}
\begin{description}
\item[$\bm{\tilde s (t,s)}$ The best $\bm{\ts{s}{0}{t+1}}$ ending in
  $\bm{s}$] Of all length $t+1$ state sequences ending in $s$ at time
  $t$, we define $\tilde s(t,s)$ to be the sequence with the highest
  joint probability with the data, \ie,
  \begin{equation*}
    \tilde s(t,s) \equiv \argmax_{ \ts{s}{0}{t+1}:\ti{s}{t}=s}
    P\left(\ts{y}{0}{t+1}, \ts{s}{0}{t+1} \right).
  \end{equation*}
\item[$\bm{\nu(t,s)}$ The \emph{utility} of the best
  $\bm{\ts{s}{0}{t+1}}$ ending in $\bm{s}$] %
  \nomenclature[gn]{$\nu(t,s)$}{Used in discussing the Viterbi
    algorithm to denote the \emph{utility} of the best sequence ending
    in state $s$.
    \begin{equation*}
      \nu(t,s) \equiv \log\left( P\left(\ts{y}{0}{t+1}, \ts{\tilde s}{0}{t+1}(s)
        \right) \right)
    \end{equation*}} % end nomenclature
  This is simply the log of the joint
  probability of the data with the best state sequence defined above,
  \ie
  \begin{equation*}
    \nu(t,s) = \log\left( P\left(\ts{y}{0}{t+1}, \tilde s(t,s)
      \right) \right)
  \end{equation*}
\item[$\bm{s'(t,s)}$ The immediate predecessor of state $\bm{s}$ in
  $\bm{\tilde s (t,s)}$] In other words, given that the best sequence
  of length $t+1$ that ends in $s_d$ is
  \begin{equation*}
    \tilde s (t,s_d) = \left[ \ti{s}{0}=s_a, \ti{s}{1}=s_b, \ldots, \ti{s}{t-1}=s_c,
      \ti{s}{t}= s_d\right]
  \end{equation*}
  the best predecessor of $s$ is the
  penultimate, \ie at time $t-1$, element of that sequence, \ie $s_c$.
\end{description}

\subsubsection*{Equations:}
Equations~\eqref{eq:cat} and \eqref{eq:nuprop} summarize the Viterbi
algorithm.  When finally deciphered, one finds Eqn.~\eqref{eq:cat} is
the vacuous statement that the best state sequence ending in $s$ at
time $t$ consists of $s$ concatenated with the best sequence ending
in $s'$ at time $t-1$, where $s'$ is the best predecessor of $s$.
\begin{equation}
  \label{eq:cat}
  \tilde s(t,s) = \left[ \tilde s(t-1,s'(t,s),s \right]
\end{equation}
Equation~\eqref{eq:nuprop} says that the total utility of the best
path of length $t+1$ to state $s$ is the utility of the best
predecessor plus two terms, one that accounts for the transition from
the predecessor to $s$ and one that accounts for the probability of
state $s$ producing the observation $\ti{y}{t}$.  From the model
assumptions (Eqn.~\eqref{eq:assume_markov} and
Eqn.~\eqref{eq:assume_output}) we find
\begin{multline*}
  P_{\ti{Y}{t+1},\ts{Y}{0}{t+1},\ti{S}{t+1},\ts{S}{0}{t+1}}
  \left(\ti{y}{t+1},\ts{y}{0}{t+1},s,\ts{s}{0}{t+1} \right) =\\
  P\left(\ts{y}{0}{t+1}, \ts{s}{0}{t+1} \right) \cdot
  P_{\ti{S}{t+1}\given \ti{S}{t}} \left(s\given \ti{s}{t} \right) \cdot
  P_{\ti{Y}{t+1}\given \ti{S}{t+1}} \left(\ti{y}{t}\given s \right).
\end{multline*}
Taking logs yields
\begin{equation}
  \label{eq:nuprop}
  \begin{split}
    \nu (t+1,s) = \nu(t,s'(t,s)) +
    \log\left(P_{\ti{S}{t+1}\given \ti{S}{}} \left(s\given s'(t,s) \right)\right)\\
    \quad + \log \left( P_{\ti{Y}{t+1}\given \ti{S}{t+1}}
      \left(\ti{y}{t}\given s\right)\right).
  \end{split}
\end{equation}

\subsubsection*{Algorithm:}
Following the steps in Fig.~\ref{fig:viterbi}, note that the algorithm
starts by assigning a utility for each state using the first
observation and then iterates forward through time.  For each time
step $t$ and each possible next state $s_\text{next}$ at time $t+1$,
the algorithm finds and records the best predecessor state $s_\text{best}$
at time $t$.  Then the algorithm calculates the utility of
$s_\text{next}$ at time $t+1$ on the basis of the utility of the best
predecessor, the conditional transition probability, and the
conditional observation probability.  At the final time $T$ the
algorithm selects the highest utility endpoint, \ie,
\begin{equation*}
  \ti{\hat s}{T-1} = \argmax_s \nu(T-1,s),
\end{equation*}
and then backtracks through the optimal predecessor links to produce
the entire highest utility path.
%%% loa: list of algorithms, toc: table of contents, lof: list of
%%% figures, lot: list of tables.
\addcontentsline{loa}{section}{Pseudocode for the Viterbi Algorithm}
%%% 
\begin{figure}[htbp]
  \begin{center}
    \def\tnext{_{\text{next}}}%
    \def\told{_{\text{old}}}%
    \def\tbest{_{\text{best}}}%
    \fbox{
      \begin{minipage}{0.90\textwidth}
        \begin{tabbing}
          XX\=XX\=XX\=XX\=XX\=XX\=XX\=XX\= \kill
          Initialize: \> \+ \\
          for each $s$\\ \> \+
          $\nu\tnext (s) = \log \left( P_{\ti{Y}{0},\ti{S}{0}}
            \left(\ti{y}{0},s \right)\right)$ \\ \\ \< \- \< \-
          Iterate: \> \+ \\
          for $t$ from 1 to $T$\\ \> \+
          Swap $\nu\tnext \leftrightarrow \nu\told$\\
          for each $s\tnext$\\ \> \+
          \\ \# Find best predecessor\\
          $s\tbest = \argmax_{s\told}\left( \nu\told(s\told) + \log\left(
              P_{\ti{S}{t}\given \ti{S}{t-1}} \left(s\tnext\given s\told \right) \right)\right)$ \\
          \\ \# Update $\nu$\\
          $\nu\tnext(s\tnext) =\,$ \= $\nu\told(s\tbest)$ \\ \> \+
          $+ \log\left( P_{\ti{S}{t}\given \ti{S}{t-1}} \left(s\tnext\given s\tbest
            \right) \right)$ \\
          $ + \log\left( P_{\ti{Y}{t}\given \ti{S}{t}} \left(\ti{y}{t}\given s\tnext \right)
          \right)  $ \\ \> \-
          \\ \# Update predecessor array\\
          Predecessor[$s\tnext,t$] = $s\tbest$\\ \\
          \< \- \< \- \< \- %This stuff is for tabs \< \-
          Backtrack: \> \+ \\
          $\ts{s}{0}{T} = \ts{\hat s}{0}{T}(\bar s)$ , where $\bar s =
          \argmax_s \nu\tnext(s)$ at $t=T-1$
        \end{tabbing}
      \end{minipage}
    }
    \caption[\comment{fig:viterbi }Pseudocode for the Viterbi Algorithm.]%
    {Pseudocode for the Viterbi Algorithm}
    \label{fig:viterbi}
  \end{center}
\end{figure}
%%%
%%% fig:viterbiB
%%%
\begin{sidewaysfigure}[htbp]
  %% 0.75\textheight is approx 5.7 in.  The .eps is exactly that size.
  %% The widths for the minipages below are taken by measuring the
  %% ovals inside of XFig.
  \centering{\plotsize%
    %% Column a
    \def\colaa{$\nu(t-1,s_1)$}%
    \def\colab{$\nu(t-1,s_2)$}%
    \def\colac{$\nu(t-1,s_3)$}%
    %% Column b
    \def\colba{$ \begin{matrix} s'(t,s_1) = \argmax_{{\tilde s}}\\%
        \log\left(P(s_1\given {\tilde s}) \right)\\+ \nu(t-1,{\tilde s})
      \end{matrix}$}%
    \def\colbb{$ \begin{matrix} s'(t,s_2) = \argmax_{{\tilde s}}\\%
        \log\left(P(s_2\given {\tilde s}) \right)\\+ \nu(t-1,{\tilde s})
      \end{matrix}$}%
    \def\colbc{$ \begin{matrix} s'(t,s_3) = \argmax_{{\tilde s}}\\%
        \log\left(P(s_3\given {\tilde s}) \right)\\+ \nu(t-1,{\tilde s})
      \end{matrix}$}%
    \def\bestpred{%
      \begin{minipage}[t]{3.9in}
        \raggedright%
        For each state $s$ find the best\\%
        predecessor $s'(t,s)$, \ie, the\\%
        one that maximizes\\%
        $\log\left(P(s\given s'(t,s)) \right) + \nu(t-1,s'(t,s))$.\\%
        The bolder lines indicate best\\%
        predecessors.
      \end{minipage}}%
    %% Column c
    \def\colca{$\nu(t,s_1)$}%
    \def\colcb{$\nu(t,s_2)$}%
    \def\colcc{$\nu(t,s_3)$}%
    \def\newnu{%
      \begin{minipage}[t]{2.7in}
        \raggedright%
        For each state $s$\\%
        calculate $\nu(t,s)$\\%
        by including the\\%
        conditional probability\\%
        of the observation $\ti{y}{t}$,\\%
        \ie, $ \nu(t,s) = \log\left(P(\ti{y}{t}\given s) \right) $\\
        \qquad$+ \log\left(P(s\given s'(t,s)) \right)$\\
        \qquad$+ \nu(t-1,s'(t,s))$.
      \end{minipage}}
    %%
    \input{viterbiB.pdf_t}}
  \vspace{7em}
  \caption[\comment{fig:viterbiB }Dependency relations in the Viterbi algorithm.]%
  {Dependency relations in the Viterbi algorithm.}
  \label{fig:viterbiB}
\end{sidewaysfigure}


\section{The Baum-Welch Algorithm}
\label{sec:baum_welch}

Given an initial vector of model parameters $\parameters$ for an HMM
and a sequence of observations $\ts{y}{0}{T}$, iteration of the
Baum-Welch algorithm \index{Baum-Welch algorithm|textbf} produces a
sequence of parameter vectors $\ts{\parameters}{1}{N}$ that almost
always converges to a local maximum of the likelihood function
$P_{\parameters} \left( \ts{y}{0}{T} \right)$.  The algorithm was
developed by \index*{Baum} and collaborators \cite{Baum70,Baum67} in
the 1960's at the Institute for Defense Analysis in Princeton.  In
each iteration, it estimates the distribution of the unobserved states
and then maximizes the expected log likelihood with respect to that
estimate.  Although Baum et al.\ limited their attention to HMMs, the
same kind of iteration works on other models that have unobserved
variables.  In 1977, Dempster Laird and Rubin \cite{Dempster77} called
the general procedure the \emph{EM algorithm}.  \index{EM
  (expectation-maximization) algorithm}%
\index{expectation-maximization (EM) algorithm}%

The EM algorithm operates on models $P_{\Y,\bS,\parameters} $ with
parameters $\parameters$ for a mix of data that is observed $(\Y)$ and
data that is unobserved $(\bS)$.  (For our application, $\Y$ is a
sequence of observations $\ts{Y}{0}{T}$ and $\bS$ is a sequence of
discrete hidden states $\ts{S}{0}{T}$.)  The steps in the algorithm
are:
\begin{enumerate}
\item Guess\footnote{Although a symmetric model in which the
    transition probability from each state to every other state is the
    same and the observation probabilities are all uniform is easy to
    describe, such a model is a bad choice for $\ti{\parameters}{1}$
    because the optimization procedure will not break the symmetry of
    the states.}  a starting value of $\ti{\parameters}{0}$, and set
  $n=0$.
\item \label{EM_loop} Choose $\ti{\parameters}{n+1}$ to maximize
  an \emph{auxiliary function} $Q$
                                %
  \begin{equation}
    \label{eq:EMmax}
    \ti{\parameters}{n+1} = \argmax_{\parameters} Q(\parameters,\ti{\parameters}{n})
  \end{equation}
  where
  \begin{equation}
    \label{eq:Qdef}
    Q(\parameters',\parameters) \equiv \EV_{P \left(\bS\given \y,\parameters \right)}
    \left( \log P(\y,\bS,\parameters') \right)
  \end{equation}
  (Here the notation $\EV_{P \left(\bS\given \y,\parameters
    \right)}(F(\bS))$ means the expected value of $F(\bS)$ over all
  values of $\bS$ using the distribution $P \left(\bS\given \y,\parameters
  \right)$.)%
  \nomenclature[rEV]{$\EV_{q(X)}(F(X))$}{The expected value of
    the function $F$ over random variable $X$ with distribution $q$.}
\item If not converged, go to~\ref{loop} with $n \leftarrow n+1.$
\end{enumerate}

We defer further discussion of the general EM algorithm to Section
\ref{sec:EM} and now proceed to the details of its application to
HMMs, \ie, the Baum-Welch algorithm.  The work of an iteration of the
EM algorithm is done in step~\ref{EM_loop}.  To apply it to an HMM, we
first characterize $P \left(\ts{s}{0}{T}\given \ts{y}{0}{T}, \parameters
\right)$ by a combination of the \emph{forward algorithm} that we
already described in Section~\ref{sec:forward} and the \emph{backward
  algorithm} which we will describe in the next section.  Using the
characterization of $P \left(\ts{s}{0}{T}\given \ts{y}{0}{T}, \parameters
\right)$, we describe the optimization specified by
Eqn.~\eqref{eq:EMmax} in Section~\ref{sec:reestimation}.

\subsection{The Backward Algorithm}
\label{sec:backward}

The \index{backward algorithm} backward algorithm is similar to the
forward algorithm in structure and complexity, but the terms are
neither as easy to interpret nor as clearly useful.  After running
both the forward algorithm and the backward algorithm, one can
calculate $P_{\ti{S}{t}\given \ts{Y}{0}{T}} \left(s\given \ts{y}{0}{T} \right)$,
the conditional probability of being in any state $s \in \states$ at
any time $t: 1\leq t \leq T$ given the entire sequence of
observations.  The forward algorithm provides the terms $\alpha(t,s)
\equiv P_{\ti{S}{t}\given \ts{Y}{0}{t}} \left(s\given \ts{y}{0}{t} \right)\,
\forall (t,s)$.  Thus the backward algorithm must provide terms, call
them $\beta(t,s)$, with the values
\begin{equation}
  \label{eq:bw1} \beta(t,s) =
  \frac{P_{\ti{S}{t}\given \ts{Y}{0}{T}} \left(s\given \ts{y}{0}{T} \right)}
  {\alpha(t,s)} = \frac{P_{\ti{S}{t}\given \ts{Y}{0}{T}}
    \left(s\given \ts{y}{0}{T} \right)}
  {P_{\ti{S}{t}\given \ts{Y}{0}{t}} \left(s\given \ts{y}{0}{t} \right)}.
\end{equation}
\nomenclature[gb]{$\beta(t,s)$}{An intermediate quantity calculated in the
  backwards algorithm which is used somewhat like $\alpha$ in the
  forward algorithm.  One may use either of the following two
  equations to define $\beta$
  \begin{align*}
    \beta(t,s) &= \frac{P_{\ts{Y}{t+1}{T} \given \ti{S}{t}}
      \left(\ts{y}{t+1}{T} \given s
      \right)} {P \left(\ts{y}{t+1}{T} \given \ts{y}{0}{t} \right)}\\
    &= \frac{P_{\ti{S}{t} \given \ts{Y}{0}{T}} \left(s \given \ts{y}{0}{T} \right)}
    {\alpha(t,s)} = \frac{P_{\ti{S}{t} \given \ts{Y}{0}{T}}
      \left(s \given \ts{y}{0}{T} \right)} {P_{\ti{S}{t} \given \ts{Y}{0}{t}}
      \left(s \given \ts{y}{0}{t} \right)}.
  \end{align*}
  The interpretation of $\beta$ is less intuitive than the
  interpretation of $\alpha$.  It is also called the backward forecast
  distribution in the Kalman filter literature.}% end nomenclature
Invoking Bayes rule and the model assumptions we find
\begin{align}
  \mlabel{eq:bw2} \beta(t,s) &= \frac{P_{\ts{Y}{0}{T},\ti{S}{t}}
    \left(\ts{y}{0}{T},s \right)\cdot P \left(\ts{y}{0}{t} \right)}
  {P_{\ts{Y}{0}{t},\ti{S}{t}}
    \left(\ts{y}{0}{t},s \right)\cdot P \left(\ts{y}{0}{T} \right)}\\
  \mlabel{eq:bw3} &= \frac{P_{\ts{Y}{t+1}{T}\given \ts{Y}{0}{t},\ti{S}{t}}
    \left(\ts{y}{t+1}{T}\given \ts{y}{0}{t},s \right)} {P
    \left(\ts{y}{t+1}{T}\given \ts{y}{0}{t} \right)}\\
  \label{eq:bw4}
  & = \frac{P_{\ts{Y}{t+1}{T}\given \ti{S}{t}} \left(\ts{y}{t+1}{T}\given s
    \right)} {P \left(\ts{y}{t+1}{T}\given \ts{y}{0}{t} \right)}.
\end{align}
(Note that if $\alpha(t,s)=0$, Eqn.~\eqref{eq:bw1} is undefined, but
we can nonetheless implement Eqn.~\eqref{eq:bw4}.)

The algorithm starts at the final time $T$ with $\beta$ set to
one\footnote{From the premises that $\alpha(t,s) \beta(t,s) =
  P_{\ti{S}{t}\given \ts{Y}{0}{T}} \left(s\given \ts{y}{0}{T} \right)$ and
  $\alpha(t,s) \equiv P_{S(T)\given \ts{Y}{0}{T}}\left(s \given \ts{y}{0}{T}
  \right)$, we conclude that $\beta(t,s) = 1 ~\forall s$.} %
for each state, $\beta(t,s) = 1,~\forall s \in \states$, and solves
for $\beta$ at earlier times with the following recursion that goes
\emph{backwards} through time:
\begin{equation}
  \label{eq:backformula}
  \beta(t-1,{\tilde s}) = \sum_{s\in\states} \beta(t,s)
  \frac{ P_{\ti{Y}{t}\given \ti{S}{t}}
    \left(\ti{y}{t}\given s \right) \cdot P_{\ti{S}{t}\given \ti{S}{t-1}}
    \left(s\given {\tilde s} \right)} {\ti{\gamma}{t}}
\end{equation}
Note that ${\ti{\gamma}{t} \equiv P \left( \ti{y}{t} \given 
    \ts{y}{0}{t}\right)}$ is calculated by the forward algorithm and
that the terms in the numerator are model parameters.  We justify
Eqn.~\eqref{eq:backformula} by using Eqn.~\eqref{eq:bw4}
\begin{align}
  \beta(t-1,{\tilde s}) & \equiv \frac{P_{\ts{Y}{t}{T}\given \ti{S}{t-1}}
    \left(\ts{y}{t}{T}\given {\tilde s} \right)}
  {P \left(\ts{y}{t}{T}\given \ts{y}{0}{t} \right)} \\
  \mlabel{eq:back2}
  &= \frac{\sum_{s\in\states} P_{\ts{Y}{t}{T},\ti{S}{t}\given \ti{S}{t-1}}
    \left(\ts{y}{t}{T},s\given {\tilde s} \right)} {P
    \left(\ts{y}{t}{T}\given \ts{y}{0}{t} \right)}.
\end{align}
Next, factor each term in the numerator of Eqn.~\eqref{eq:back2} using
Bayes rule twice then apply the model assumptions and the expression
for $\beta(t,s)$ from Eqn.~\eqref{eq:bw4}:
\begin{align*}
  & P_{\ts{Y}{t}{T},\ti{S}{t}\given \ti{S}{t-1}}\left( \ts{y}{t}{T},s\given {\tilde s} \right)\\
  \begin{split}
    &\quad= P_{\ts{Y}{t+1}{T}\given \ti{Y}{t},\ti{S}{t},\ti{S}{t-1}}
       \left(\ts{y}{t+1}{T}\given \ti{y}{t},s,{\tilde s} \right)\\
    &\quad\qquad \cdot P_{\ti{Y}{t}\given \ti{S}{t},\ti{S}{t-1}} \left(\ti{y}{t}\given s,{\tilde s}
       \right)\cdot P_{\ti{S}{t}\given \ti{S}{t-1}} \left(s\given {\tilde s} \right)
  \end{split}\\
  &\quad= P_{\ts{Y}{t+1}{T}\given \ti{S}{t}} \left(\ts{y}{t+1}{T}\given s\right)
           \cdot P_{\ti{Y}{t}\given \ti{S}{t}} \left(\ti{y}{t}\given s \right) \cdot
            P_{\ti{S}{t}\given \ti{S}{t-1}} \left(s\given {\tilde s} \right) \\
  &\quad= \beta(t,s) \cdot P \left(\ts{y}{t+1}{T}\given \ts{y}{0}{t} \right)
           \cdot P_{\ti{Y}{t}\given \ti{S}{t}} \left(\ti{y}{t}\given s \right) \cdot
            P_{\ti{S}{t}\given \ti{S}{t-1}} \left(s\given {\tilde s} \right)
\end{align*}
Similarly, simplify the denominator of Eqn.~\eqref{eq:back2} using
Bayes rule:
\begin{equation*}
  P \left(\ts{y}{t}{T}\given \ts{y}{0}{t} \right) = P
  \left(\ts{y}{t+1}{T}\given \ts{y}{0}{t}  \right) \cdot {P \left(
  \ti{y}{t}\given \ts{y}{0}{t}\right)}
\end{equation*}
Finally by substituting these values into the fraction of
\eqref{eq:back2}, we verify the recursion \eqref{eq:backformula}
\begin{align}
  \mlabel{eq:back3} \beta(t-1,{\tilde s}) &= \frac{\sum_{s\in\states}
    \beta(t,s) \cdot P \left(\ts{y}{t+1}{T}\given \ts{y}{0}{t} \right)
    \cdot P_{\ti{Y}{t}\given \ti{S}{t}} \left(\ti{y}{t}\given s \right) \cdot
    P_{\ti{S}{t}\given \ti{S}{t-1}} \left(s\given {\tilde s} \right)} {P
    \left(\ts{y}{t+1}{T}\given \ts{y}{0}{t} \right) \cdot {P \left(
        \ti{y}{t}\given \ts{y}{0}{t}\right)}} \\
  \mlabel{eq:back4} &= \sum_{s\in\states} \beta(t,s) \frac{
    P_{\ti{Y}{t}\given \ti{S}{t}} \left(\ti{y}{t}\given s \right) \cdot
    P_{\ti{S}{t}\given \ti{S}{t-1}} \left(s\given {\tilde s} \right)} {\ti{\gamma}{t}}.
\end{align}
By introducing the intermediate quantity $b(t,s)$ we can break
\eqref{eq:back4} into the two steps
\begin{subequations}
  \label{eq:2LineBackward}
  \begin{align}
    \mlabel{eq:back5}
    b(s,t) &= \frac{\beta(t,s) \ti{y}{t}}{\ti{\gamma}{t}} \\
    \mlabel{eq:back6}
    \beta(t-1,\tilde s) &= \sum_{s\in\states} b(s,t) P_{\ti{S}{1}\given
                          \ti{S}{0}} \left(s\given {\tilde s} \right),
  \end{align}
\end{subequations}
which clarifies that, aside from normalization, the backward algorithm
like the forward algorithm alternates between multiplying a state
distribution by an observation likelihood and then multiplying the
result by the transition probability matrix or its transpose.

\subsection{Weights and Reestimation}
\label{sec:reestimation}

Each pass of the Baum-Welch algorithm consists of the following steps:
Run the forward algorithm described in Section~\ref{sec:forward} to
calculate the values of $\alpha(t,s)$ and $\gamma(t)$ for each time $t
\in [0,\ldots,T-1]$ and each state $s \in \states$; Run the backward
algorithm described in Section~\ref{sec:backward} to calculate the
values of $\beta(t,{\tilde s})$; Reestimate the model parameters using the
formulas in Table~\ref{tab:reestimation}.

We write the reestimation formulas in terms of \emph{weights} which
express the conditional probability of being in specific states at
specific times given the observed data $\ts{y}{0}{T}$.  We denote the
conditional probability of being in state $s$ at time $t$ given all of
the data by
\begin{equation}
  \label{eq:wit}
  w(t,s) \equiv P_{\ti{S}{t}\given \ts{Y}{0}{T}} \left(s\given \ts{y}{0}{T}
  \right),
\end{equation}
\nomenclature[rwst]{$w(t,s)$}{In the reestimation phase of the Baum-Welch
  algorithm, the \emph{weight} assigned to state $s$ at time $t$,
  \begin{equation*}
    w(t,s) \equiv P_{\ti{S}{t} \given \ts{Y}{0}{T}} \left(s \given \ts{y}{0}{T}
    \right).
  \end{equation*}}% end nomenclature
and we denote the conditional probability, given all of the data, of
being in state $s$ at time $t$ and being in state ${\tilde s}$ at time
$t+1$ by
\begin{equation}
  \label{eq:wijt}
  {\tilde w}(t,{\tilde s},s) \equiv P_{\ti{S}{t+1},\ti{S}{t}\given \ts{Y}{0}{T}}
  \left({\tilde s},s\given \ts{y}{0}{T} \right).
\end{equation}
\nomenclature[rwsst]{$ {\tilde w}(t,{\tilde s},s)$}{In the reestimation phase
  of the Baum-Welch algorithm, the \emph{weight} assigned to the
  transition from state $s$ at time $t$ to state $\tilde s$ at time $t+1$,
  \begin{equation*}
     {\tilde w}(t,{\tilde s},s) \equiv P_{\ti{S}{t+1},\ti{S}{t} \given \ts{Y}{0}{T}}
  \left({\tilde s},s \given \ts{y}{0}{T} \right).
  \end{equation*}}
Table~\ref{tab:reestimation} (page \pageref{tab:reestimation})
summarizes the formulas for the updated model parameters after one
pass of the Baum-Welch algorithm.

To derive reestimation formulas for $P_{S(1)}$ and
$P_{\ti{Y}{t}\given \ti{S}{t}}$ we will consider a sum over all possible
state sequences $\ts{s}{0}{T}$, \ie,
\begin{equation}
  \label{eq:witFFE}
  w(t,s) = \sum_{\ts{s}{0}{T}:\ti{s}{t}=s}
  P \left(\ts{s}{0}{T}\given \ts{y}{0}{T}
  \right).
\end{equation}
Since this is virtually unimplementable, the actual algorithm uses
\begin{equation}
  \label{eq:witalphabeta}
  w(t,s) = \alpha(t,s) \beta(t,s).
\end{equation}
In fact, in Eqn.~\eqref{eq:bw1}, we chose the expression for $\beta$
to make Eqn.~\eqref{eq:witalphabeta} true.

Similarly, we will derive the reestimation formula for
$P_{\ti{S}{t+1}\given \ti{S}{t}}$ using
\begin{equation}
  \label{eq:wijtFFE}
  {\tilde w}(t,{\tilde s},s) = \sum_{\substack{\ts{s}{0}{T}:\ti{s}{t+1}={\tilde s},\\\ti{s}{t}=s}}
  P \left(\ts{s}{0}{T}\given \ts{y}{0}{T} \right),
\end{equation}
but in the algorithm we use
\begin{equation}
  \label{eq:wijtalphabeta}
  {\tilde w}(t,{\tilde s},s) = \frac{ \alpha(t,s) \cdot P_{\ti{S}{t+1}\given \ti{S}{t}}
    \left({\tilde s}\given s \right)  \cdot P_{\ti{Y}{t+1}\given \ti{S}{t+1}}
    \left(\ti{y}{t+1}\given {\tilde s} \right) \cdot \beta(t+1,{\tilde s}) }
  {\ti{\gamma}{t+1}}.
\end{equation}
One can verify Eqn.~\eqref{eq:wijtalphabeta} using the model
assumptions, the definitions of $\alpha$, $\beta$, and $\gamma$, and
Bayes rule.

\subsubsection{Reestimation}

With Eqns.~\eqref{eq:witalphabeta} and \eqref{eq:wijtalphabeta} for
$w(t,i)$ and ${\tilde w}(t,{\tilde s},s)$ in terms of known quantities
($\alpha$, $\beta$, $\gamma$, $\ts{y}{0}{T}$, and the old model
parameters $\ti{\parameters}{n}$), we are prepared to use the formulas
in Table \ref{tab:reestimation} to calculate new estimates of the
model parameters, $\ti{\parameters}{n+1}$ with higher likelihood.

\begin{table}[htbp]
  \caption[\comment{tab:reestimation }Summary of reestimation formulas.]%
  {Summary of reestimation formulas.\index{reestimation formulas}}
  \centering{\plotsize%
    \begin{minipage}{.7\textwidth}
      Note that formulas for $w(t,s)$ and ${\tilde w}(t,{\tilde s},s)$
      appear in Eqns.~\eqref{eq:witalphabeta} and
      \eqref{eq:wijtalphabeta} respectively.
    \end{minipage}\\[1ex]
%    \begin{tabular*}{0.98\textwidth}[H]{|l|r|l|}
    \begin{tabular}[H]{|c|c|c|}
      \hline
      \rule{0pt}{2.5ex}Description & Expression & New Value \\
      \hline
      \rule{0pt}{2.5ex}Initial State Prob.
      & $P_{\ti{S}{0}\given \ti{\parameters}{n+1}} \left(s\given \ti{\parameters}{n+1} \right)$
      & $ w(0,s) $ \\[1.5ex]
      State Transition Prob.
      & $P_{\ti{S}{t+1}\given \ti{S}{t},\ti{\parameters}{n+1}} \left({\tilde
          s}\given s, \ti{\parameters}{n+1} \right)$
      & $ \frac {\sum_{t=1}^{T-1} {\tilde w}(t,{\tilde s},s)} {\sum_{s'\in\states}
        \sum_{t=1}^{T-1} {\tilde w}(t,s',s)}$ \\[2.5ex]
      Cond. Observation Prob.
      & $P_{\ti{Y}{t}\given \ti{S}{t},\ti{\parameters}{n+1}} \left(y\given s, \ti{\parameters}{n+1} \right)$
      & $ \frac {\sum_{t:\ti{y}{t}=y} w(t,s)} {\sum_{t} w(t,s)}$ \\[2.0ex]
      \hline
%    \end{tabular*}}
    \end{tabular}}
  \label{tab:reestimation}
\end{table}

To derive the formulas in Table \ref{tab:reestimation}, start with
Step 2 of the EM algorithm (see Eqn.~\eqref{eq:Qdef}) which is to
maximize the auxiliary function
\begin{equation*}
  Q(\parameters',\parameters) \equiv \EV_{P \left(\bS\given \y \right),\parameters}
  \left( \log P(\y,\bS\given \parameters') \right)
\end{equation*}
with respect to $\parameters'$.  For an HMM, substitute the sequence of
\emph{hidden} states $\ts{s}{0}{T}$ for $\bS$ and the sequence of
observations $\ts{y}{0}{T}$ for $\y$.  Note that the joint probability
of a state sequence $\ts{s}{0}{T}$ and the observation sequence
$\ts{y}{0}{T}$ is
\begin{equation*}
  P\left( \ts{s}{0}{T},\ts{y}{0}{T} \right) = P_{\ti{S}{0}}
  \left(\ti{s}{0} \right) \cdot \prod_{t=1}^{T-1} P_{\ti{S}{1}\given \ti{S}{0}}
  \left(\ti{s}{t}\given \ti{s}{t-1} \right)
  \cdot \prod_{t=0}^{T-1}  P_{\ti{Y}{0}\given \ti{S}{0}}
  \left(\ti{y}{t}\given \ti{s}{t} \right),
\end{equation*}
or equivalently,
\begin{align*}
  \log P\left(\ts{y}{0}{T}, \ts{s}{0}{T} \right) &= \log P_{\ti{S}{0}}
  \left(\ti{s}{0} \right) + \sum_{t=1}^{T-1} \log P_{\ti{S}{1}\given \ti{S}{0}}
  \left(\ti{s}{t}\given \ti{s}{t-1} \right)\\
  &\quad + \sum_{t=0}^{T-1} \log  P_{\ti{Y}{0}\given \ti{S}{0}}
  \left(\ti{y}{t}\given \ti{s}{t} \right).
\end{align*}
We can optimize $Q$ by breaking it into a sum in which each of the
model parameters only appears in one of the terms and then optimizing
each of the terms independently:
\begin{align}
  \label{eq:QHMM}
  Q(\parameters',\parameters) &= \sum_{\ts{s}{0}{T}\in\states^T}
  \left( P
  \left(\ts{s}{0}{T}\given \ts{y}{0}{T},\parameters \right) \log P
  \left(\ts{s}{0}{T},\ts{y}{0}{T},\parameters' \right) \right) \\
  \label{eq:QHMMseparate}
  & \equiv Q_{\text{initial}} (\parameters',\parameters) +
  Q_{\text{transition}} (\parameters',\parameters) + Q_{\text{observation}}
  (\parameters',\parameters),
\end{align}
where
\begin{align}
  \label{eq:Qinitial}
  Q_{\text{initial}} (\parameters',\parameters) &\equiv
%
  \sum_{\ts{s}{0}{T}\in\states^T} \left( P
  \left(\ts{s}{0}{T}\given \ts{y}{0}{T},\parameters \right) \log P_{\ti{S}{0}\given \parameters'}
  \left(\ti{s}{0}\given \parameters' \right) \right)\\
%
  \label{eq:Qtransition}
  Q_{\text{transition}} (\parameters',\parameters) &\equiv
%
  \sum_{\ts{s}{0}{T}\in\states^T} \left( P
  \left(\ts{s}{0}{T}\given \ts{y}{0}{T},\parameters \right) \sum_{t=1}^{T-1} \log
  P_{\ti{S}{1}\given \ti{S}{0},\parameters'}
  \left(\ti{s}{t}\given \ti{s}{t-1},\parameters' \right) \right) \\
%
  \label{eq:Qoutput}
  Q_{\text{observation}} (\parameters',\parameters) &\equiv
%
  \sum_{\ts{s}{0}{T}\in\states^T} \left( P
  \left(\ts{s}{0}{T}\given \ts{y}{0}{T},\parameters \right) \sum_{t=1}^T \log
  P_{\ti{Y}{0}\given \ti{S}{0},\parameters'}
  \left(\ti{y}{t}\given \ti{s}{t},\parameters' \right) \right)
%
\end{align}

To simplify the appearance of expressions as we optimize $Q$, we
introduce notation for logs of parameters
\begin{align}
  L_{\text{initial}}(i) & \equiv \log P_{\ti{S}{0}\given \parameters'}
  \left(i\given \parameters' \right) \\
  L_{\text{transition}}(i,j) &\equiv \log P_{\ti{S}{1} \given 
    \ti{S}{0},\parameters'}({i}\given j,\parameters') \\
  \label{eq:Loutput}
  L_{\text{observation}}(y,i) & \equiv \log
  P_{\ti{Y}{0}\given \ti{S}{0},\parameters'} \left(y\given i,\parameters' \right).
\end{align}
Now to optimize $Q_{\text{initial}}$ write Eqn.~\eqref{eq:Qinitial} as
\begin{align}
  Q_{\text{initial}} (\parameters',\parameters) &=
  \sum_{\ts{s}{0}{T}\in\states^T} P
  \left(\ts{s}{0}{T}\given \ts{y}{0}{T},\parameters \right)
  L_{\text{initial}}(\ti{s}{0})\\
  &= \sum_{s \in \states} L_{\text{initial}}(s)
  \sum_{\ts{s}{0}{T}:\ti{s}{0}=s} P
  \left(\ts{s}{0}{T}\given \ts{y}{0}{T},\parameters \right)\\
  &= \sum_s L_{\text{initial}}(s) P_{\ti{S}{0}\given \ts{Y}{0}{T},\parameters}
  \left(s\given \ts{y}{0}{T},\parameters \right) \text{ see Eqn.~\eqref{eq:witFFE}}\\
  &= \sum_s L_{\text{initial}}(s) w(0,s)
\end{align}
We wish to find the set $\left\{ L_{\text{initial}}(s) \right\}$ that
maximizes $Q_{\text{initial}} (\parameters',\parameters)$ subject to the
constraint
\begin{equation*}
  \sum_s e^{ L_{\text{initial}}(s)} \equiv\sum_s
  P_{\ti{S}{0}\given \hat \parameters} \left(s\given \hat \parameters \right) = 1.
\end{equation*}
The method of Lagrange multipliers yields
\begin{equation}
  \label{eq:LiSol}
   L_{\text{initial}}(s) = \log w(0,s) ~ \forall s,
\end{equation}
ie, the new estimates of the initial probabilities are
\begin{equation}
  \label{eq:InitialRe}
  P_{\ti{S}{0}\given \ti{\parameters}{n+1}} \left(s\given \ti{\parameters}{n+1} \right) = w(0,s).
\end{equation}

To derive new estimates of the state transition probabilities, write
\begin{align}
  Q_{\text{transition}} (\parameters',\parameters) &=
  \sum_{\ts{s}{0}{T}\in\states^T} P
  \left(\ts{s}{0}{T}\given \ts{y}{0}{T},\parameters \right) \sum_{t=1}^{T-1}
  L_{\text{transition}}(\ti{s}{t-1},\ti{s}{t})\\
  &= \sum_{s,{\tilde s}} L_{\text{transition}}(s,{\tilde s})
  \sum_{t=1}^{T-1} \sum_{\substack{\ts{s}{0}{T}:\ti{s}{t}={\tilde
        s},\\\ti{s}{t-1}=s}}
  P\left(\ts{s}{0}{T}\given \ts{y}{0}{T},\parameters \right)\\
  &= \sum_{s,{\tilde s}} L_{\text{transition}}(s,{\tilde s})
  \sum_{t=1}^{T-1} {\tilde w}(t,{\tilde s},s).
\end{align}
Optimization yields
\begin{equation}
  \label{eq:NewTrans}
  P_{\ti{S}{1}\given \ti{S}{0},\ti{\parameters}{n+1}} \left({\tilde
      s}\given s,\ti{\parameters}{n+1} \right) = \frac
  {\sum_{t=1}^{T-1} {\tilde w}(t,{\tilde s},s)} {\sum_{s'}
    \sum_{t=1}^{T-1} {\tilde w}(t,s',s)}.
\end{equation}

Similarly, we derive the new estimates of the conditional observation
probabilities from
\begin{align}
  \label{eq:Qout1}
  Q_{\text{observation}} (\parameters',\parameters) &=
  \sum_{\ts{s}{0}{T}\in\states^T} P
  \left(\ts{s}{0}{T}\given \ts{y}{0}{T},\parameters \right) \sum_{t=1}^{T-1}
  L_{\text{observation}}(\ti{y}{t},\ti{s}{t})\\
  &= \sum_{y\in\outputs,s\in\states} L_{\text{observation}}(y,s)
  \sum_{t:\ti{y}{t}=y}
  \sum_{\ts{s}{0}{T}:\ti{s}{t}=s }P
  \left(\ts{s}{0}{T}\given \ts{y}{0}{T},\parameters \right) \\
  &= \sum_{y,s} L_{\text{observation}}(y,s) \sum_{t:\ti{y}{t}=y} w(t,s).
\end{align}
Optimization yields
\begin{equation}
  \label{eq:NewOut}
  P_{\ti{Y}{t}\given \ti{S}{t},\ti{\parameters}{n+1}}
  \left(y\given s,\ti{\parameters}{n+1} \right) = \frac
  {\sum_{t:\ti{y}{t}=y} w(t,s)} {\sum_{t=1}^{T-1} w(t,s)}.
\end{equation}

%%%
%%% fig:train
%%%
\addcontentsline{loa}{section}{Baum-Welch model parameter optimization}
\begin{figure}[htbp]
  \begin{center}
    %\small%
    \def\assign{\leftarrow}%
    \def\oldmodel{\ti{\parameters}{n}}%
    \def\newmodel{\ti{\parameters}{n+1}}%
% Tabbing commands:
% \=    Set a stop
% \>    Skip to the next stop
% \<    Go back a stop
% \\    New line
% \+    Move left margin right one stop
% \-    Move the left margin left one stop
\fbox{
  \begin{minipage}{0.90\textwidth}
    \begin{tabbing}
      XX\=XX\=XX\=XX\=XX\=XX\=XX\=XX\= \kill
      Notation: \> \+ \\ \\
      \begin{minipage}[b]{1.0\textwidth}
        $\oldmodel$ is the model, or equivalently the set of
        parameters, after $n$ iterations of the Baum-Welch algorithm.
      \end{minipage}\\ \\%
      %%
      \begin{minipage}[b]{1.0\textwidth}
        $\bm{\alpha}_n$ is the set of conditional state probabilities
        calculated on the basis of the $n^{\text{th}}$ model and the
        data $\ts{y}{0}{T}$.  See Eqns.~\eqref{eq:alpha} and
        \eqref{eq:forwardD}.
        \begin{equation*}
          \bm{\alpha}_n \equiv \left\{ P_{ S(t)\given  \ts{Y}{0}{t},\oldmodel} \left( s \given 
             \ts{y}{0}{t},\oldmodel\right) : \forall s \in \states\, \& \, 0
           \leq t < T \right\}
        \end{equation*}
      \end{minipage}\\ \\%
      %%
      \begin{minipage}[b]{1.0\textwidth}
         $\bm{\beta}_n$ is a set of values calculated on the basis of the
         $n^{\text{th}}$ model $\oldmodel$ and the data $\ts{y}{0}{T}$.  See
         Eqns.~\eqref{eq:bw4} and \eqref{eq:backformula}.
         \begin{equation*}
         \bm{\beta}_n \equiv \left\{ \frac{P_{\ts{Y}{t+1}{T}\given \ti{S}{t}}
         \left(\ts{y}{t+1}{T} \given  s \right)} {P
         \left(\ts{y}{t+1}{T}\given \ts{y}{0}{t} \right)} : \forall s \in
         \states\, \& \, 0 \leq t < T \right\}
         \end{equation*}
      \end{minipage}\\ \\%
      %%
      \begin{minipage}[b]{1.0\textwidth}
         $\bm{\gamma}_n$ is the set of conditional observation probabilities
         calculated on the basis of the $n^{\text{th}}$ model $\oldmodel$ and
         the data $\ts{y}{0}{T}$.  See Eqns.~\eqref{eq:gamma} and
         \eqref{eq:forwardC}.
         \begin{equation*}
         \bm{\gamma}_n \equiv \left\{P \left(\ti{y}{t} \given  \ts{y}{0}{t},
         \oldmodel\right) : \, 0 \leq t < T \right\}
         \end{equation*}
      \end{minipage}\\ \\%
      %%
      \<\-
      Initialize: \> \+ \\
      Set $n=0$ and choose $\ti{\parameters}{0}$\\ \\ \< \-
      Iterate: \> \+ \\
      $\newmodel \assign \text{reestimate} \left( \ts{y}{0}{T},
      \bm{\alpha}_n, \bm{\beta}_n, \bm{\gamma}_n, \oldmodel\right)$ XX\= \kill
      $\left(\bm{\alpha}_n,\bm{\gamma}_n\right) \assign
      \text{forward}(\ts{y}{0}{T},\oldmodel)$ \> See Section
      \ref{sec:forward} page \pageref{sec:forward}\\
      $\bm{\beta}_n \assign \text{backward}(\bm{\gamma}_n, \ts{y}{0}{T},
      \oldmodel)$ \> See Section \ref{sec:backward}  page \pageref{sec:backward}\\
      $\newmodel \assign \text{reestimate} \left( \ts{y}{0}{T},
      \bm{\alpha}_n, \bm{\beta}_n, \bm{\gamma}_n, \oldmodel\right)$ \> See Table
      \ref{tab:reestimation} page \pageref{tab:reestimation} \\
      $n \assign n+1$ \\
      Test for completion
    \end{tabbing}
  \end{minipage}
}
\caption[\comment{fig:train }Baum-Welch model parameter optimization.]%
{Summary and pseudo-code for optimizing model parameters by iterating
  the Baum-Welch algorithm. \index{Baum-Welch algorithm}}
    \label{fig:train}
  \end{center}
\end{figure}

\section{Remarks}
\label{sec:AlgApp}

\subsection{MAP Sequence of States or Sequence of MAP States?}
\label{sec:sequenceMAP}

\index{sequence of maximum a posteriori state estimates}%

Consider the difference between the sequence of maximum a posteriori states
and the maximum a posteriori sequence of states.  The maximum a posteriori state
at a particular time $t$ is the best guess for where the system was at
that time given all of the observations, \ie
%
$\ti{\hat s}{t} = \argmax_{s'}P(\ti{s'}{t}\given \ts{y}{0}{T}) =
\argmax_{s'}\alpha(t,s') \beta(t,s')$
%
(see \eqref{eq:wit} --
\eqref{eq:witalphabeta}).  While it seems reasonable that a sequence
of such guesses would constitute a good guess for the entire
trajectory, it is not an optimal trajectory estimate.  In fact, as the
following example demonstrates, such a trajectory may even be
impossible.

Consider the HMM drawn in Fig.~\ref{fig:sequenceMAP} and the sequence
of observations $\ts{y}{0}{6} = (a,b,b,b,b,c)$.  Any sequence of
states that is consistent with $\ts{y}{0}{6}$ must begin in $e$, end
in $g$, and pass through state $f$ exactly once.  The only unknown
remaining is the time at which the system was in state $f$.  Here is a
tabulation of the four possible state sequences:
\begin{center}
  \begin{tabular}{|cccccc|l|c|}
    \hline
    $\ti{s}{1}$ &  $\ti{s}{2}$ &  $\ti{s}{3}$ & 
    $\ti{s}{4}$ &  $\ti{s}{5}$ &  $\ti{s}{6}$ &
    $P(\ts{y}{0}{6},\ts{s}{0}{6})/z$ & 
    $P(\ts{s}{0}{6}\given \ts{y}{0}{6})$ \\
    \hline
    $e$ & $e$ & $e$ & $e$ & $f$ & $g$ & $0.9^3$           & $ 0.30$ \\
    $e$ & $e$ & $e$ & $f$ & $g$ & $g$ & $0.9^2\cdot 0.8$  & $ 0.26$ \\
    $e$ & $e$ & $f$ & $e$ & $g$ & $g$ & $0.9 \cdot 0.8^2$ & $ 0.23$ \\
    $e$ & $f$ & $g$ & $g$ & $g$ & $g$ & $0.8^3$           & $ 0.21$ \\\hline
  \end{tabular}
\end{center}
In the table, the term $z$ represents the factors that are common in
$P(\ts{y}{0}{6},\ts{s}{0}{6})$ for all of the possible state
sequences.  Only the factors that are different appear in the seventh
column.  The largest entry in the last column, where only two
significant figures appear, is 0.30 which corresponds to the MAP
estimate: $\ts{\hat s}{1}{6} = (e,e,e,e,f,g,)$.
\begin{figure}[htbp]
  \centering{\plotsize%
    \input{sequenceMAP.pdf_t} 
  }  
  \caption{HMM used to illustrate that the maximum a posteriori sequence of states is
    not the same as the sequence of maximum a posteriori states.}
\label{fig:sequenceMAP}
\end{figure}

The next table displays the values of $P(\ti{s}{t}\given \ts{y}{0}{6})$, the
a posteriori probability for the three possible states:
\begin{center}
  \begin{tabular}{cc|cccccc}
    &   & \multicolumn{6}{c}{$t$} \\
    \multicolumn{2}{r|}{$P(\ti{s}{t}\given \ts{y}{0}{6})$}
        & 1 & 2 & 3 & 4 & 5 & 6 \\
    \hline
    & $e$ & \textbf{1.0} & \textbf{0.79} & \textbf{0.56} & 0.30 & 0    & 0  \\
$s$ & $f$ & 0   & 0.21 & 0.23 & 0.26 & 0.30 & 0   \\
    & $g$ & 0   & 0    & 0.21 & \textbf{0.44} & \textbf{0.70} & \textbf{1.0}
  \end{tabular}
\end{center}
The table quantifies the intuition that the a posteriori probability starts
in state $e$ at time $t=1$ and sort of diffuses completely over to
state $g$ by time $t=6$.  Notice that although all of the probability
passes through state $f$, at no time is it the most probable state.
Thus the sequence of maximum a posteriori states is $e,e,e,g,g,g$ which is
an \emph{impossible} sequence.  On the other hand, the maximum a posteriori
sequence of states, $e,e,e,e,f,g$, is entirely plausible.

\subsection{Training on Multiple Segments}
\label{sec:MultiSegment}

\index{training on multiple segments}
\index{multiple segments, training on}

Simple modifications to code for the Baum-Welch algorithm enable it to
train on data that consists of a collection of independent segments
$\mathbf{\vec y} \equiv \left\{ \mathbf{y}_0, \mathbf{y}_1, \ldots,
  \mathbf{y}_{n-1} \right\}$ where $ \mathbf{y}_k = \ts{y_k}{0}{T_k}$.
In particular for each iteration, one should:
\begin{itemize}
\item Run the forward and backward algorithms on each segment
  $\mathbf{y}_k$ to calculate ${\bm \alpha}_k$ and
  ${\bm \beta}_k$
\item Create ${\bm \alpha}$ and ${\bm \beta}$ by concatenating
  ${\bm \alpha}_k~\forall k$ and ${\bm \beta}_k~\forall k$
  respectively.
\item Reestimate all model parameters by applying the formulas in
  Table~\ref{tab:reestimation} to the concatenated  ${\bm \alpha}$,
  ${\bm \beta}$, and $\mathbf{\vec y}$.
\item Modify the reestimated initial state probabilities using
  \begin{equation*}
    P_{\ti{S}{0}\given \ti{\parameters}{m+1}} \left(s\given \ti{\parameters}{m+1}
    \right) = \frac{1}{n} \sum_{k=0}^{n-1} \alpha_k(s,0) \beta_k(0,s)
  \end{equation*}
\end{itemize}

\subsection{Probabilities of the initial state}
\label{sec:Ps0}

Using the procedure of the previous section for a few independent
observation sequences with several iterations of the Baum-Welch
algorithm produces a model in which the estimates of the probabilities
of the initial states, $P_{\ti{S}{0}} \left(s \right)\,\forall s \in
\states$, reflect the characteristics at the beginning of the given
sequences.  Those estimates are appropriate if all observation
sequences come from state sequences that start in a similar fashion.
Such models are not \index*{stationary}.  To accommodate the many
applications in which we wish to model the state dynamics as
\index*{stationary}, we also calculate stationary initial state probability
estimates using
\begin{equation}
  \label{eq:Ps0stationary}
  P_{\ti{S}{0}(\text{stationary})} \left(s \right) =
  \frac{\sum_{t}w(t,i)}{\sum_{j,t}w(t,j)}
\end{equation}



\subsection{Maximizing likelihood over unrealistic classes}
\label{sec:incredible}

We often fit simple hidden Markov models to data that come from
systems that have complicated continuous state spaces.  For example,
in Chapter~\ref{chap:apnea} we fit models with roughly 10 states to
electrocardiograms even though we believe that partial differential
equations over vector fields better describe physiological dynamics
that affect the signal.  By fitting unrealistically simple models we
reduce the variance of the parameter estimates at the expense of
having less accurate models and parameters that are harder to
interpret.  It is a version of the classic \index{bias-variance
  trade-off} bias-variance trade-off.

\subsection{Multiple Local Maxima}
\label{sec:MultiMax}

The Baum-Welch algorithm generically converges to a \emph{local}
maximum of the likelihood function.  For example, we obtained the
model used to generate Fig.~\ref{fig:Statesintro} by iterating the
Baum-Welch algorithm on an initial model with random parameters.  By
re-running the experiment with five different seeds for the random
number generator, we obtained the five different results that appear
in Fig.~\ref{fig:TrainChar}.
\begin{figure}
  \centering
  % \resizebox{\textwidth}{!}{\includegraphics{TrainChar.pdf}}
  % The result of resizebox seems to be the same as the
  % includegraphics below.
  \includegraphics[width=1.0\textwidth]{TrainChar.pdf}
  \caption[\comment{fig:TrainChar }Convergence of the Baum-Welch algorithm.]{%
    Convergence of the Baum-Welch algorithm.  Here we have
    plotted the log likelihood per step as a function of the number of
    iterations $n$ of the Baum-Welch algorithm for five different
    initial models $\ti{\parameters}{0}$.  We used the same sequence
    of observations $\ts{y}{0}{T}$ that we used for
    Fig.~\ref{fig:Statesintro}, and we used different seeds for a
    random number generator to make the five initial models.  Note the
    following characteristics: The five different initial models all
    converge to different models with different likelihoods; The curves
    intersect each other as some models improve more with training
    than others; Convergence is difficult to determine because some
    curves seem to have converged for many iterations and later rise
    significantly.  }
  \label{fig:TrainChar}
\end{figure}

\subsection{Disappearing Transitions}
\label{sec:disappear}

For some observation sequences $\ts{y}{0}{T}$ and initial models,
multiple iterations of the Baum-Welch algorithm leads to state
transition probabilities that are too small to be represented in
double precision.  In our implementation of the Baum-Welch algorithm,
we set those transition probabilities to zero.  Such pruning:
\begin{itemize}
\item Prevents numerical underflow exceptions
\item Simplifies the models
\item Lets the code that process the models run faster when we use
  sparse matrices
\end{itemize}
There are methods (see for example \cite{Ormoneit95}) for addressing
situations in which one believes that some transitions should be
allowed by a model even though maximizing the likelihood by the
Baum-Welch algorithm would numerically drive their probability to
zero.

\subsection{Bayesian Estimates Instead of Point Estimates}
\label{sec:EstWholeDist}

A Bayesian parameter estimation scheme begins with an \emph{a priori}
distribution $P_{\parameters}$ that characterizes knowledge about what
values are possible and then uses Bayes rule to combine that knowledge
with observed data $y$ to calculate an \emph{a posteriori}
distribution of parameters $P_{\parameters\given y}$.  We don't really
believe that the maximum likelihood estimate (MLE) produced by the
Baum-Welch algorithm is precisely the \emph{one true answer}.  It is
what Bayesians call a \emph{point estimate}.  Parameter values near
the MLE are just as plausible given the data.  A proper Bayesian
procedure characterizes the plausibility of other parameter values
with an \emph{a posteriori} distribution.  In the next chapter, we
present a variant on the Baum-Welch algorithm that uses a prior
distribution on parameter values and produces an estimate that
maximizes the \emph{a posteriori} probability, \ie, a \emph{MAP}
estimate.  However, like the MLE, the MAP is a point estimate.
Neither does a good job of characterizing the set of plausible
parameters.

In addition to yielding only a point estimate, the Baum-Welch
algorithm is indirect in that each pass optimizes an auxiliary
function rather than optimizing the likelihood, and it converges to
\emph{local} maxima.  A Bayesian \emph{Markov chain Monte Carlo}
approach would address all of these objections at the expense of being
slow.

Although others have obtained Bayesian \emph{a posteriori} parameter
distributions for HMMs using \emph{Markov chain Monte Carlo} and
\emph{variational Bayes} procedures (see for example \cite{Rosales04}
and \cite{Beal03}), we will restrict our attention to point estimates
from the Baum-Welch algorithm and simple variations.


\section{The EM algorithm}
\label{sec:EM}
\index{EM (expectation-maximization) algorithm|textbf}%
\index{expectation-maximization (EM) algorithm|textbf}%

In Section \ref{sec:baum_welch}, we described the Baum-Welch algorithm
for finding parameters of an HMM that maximize the likelihood, and we
noted that the Baum-Welch algorithm is a special case of the EM
algorithm.  Here, we examine the general EM algorithm in more detail.
Readers willing to accept the Baum-Welch algorithm without further
discussion of Eqn.~\ref{eq:EMmax} should skip this section.  Dempster
Laird and Rubin\cite{Dempster77} coined the term \emph{EM algorithm}
in 1977.  More recent treatments include Redner and
Walker\cite{Redner84}, McLachlan and Krishnan\cite{McLachlan96} and
Watanabe and Yamaguchi\cite{Watanabe04}.  \marginpar{Want 2024 recent}
%
Recall that the algorithm operates on models $P_{\Y,\bS,\parameters} $
with parameters $\parameters$ for a mix of data that is observed
$(\Y)$ and data that is unobserved $(\bS)$ and that the steps in the
algorithm are:
\begin{enumerate}
\item Guess a starting value of $\ti{\parameters}{0}$, and set $n=0$.
\item \label{loop} Choose $\ti{\parameters}{n+1}$ to maximize
  an \emph{auxiliary function} $Q$
                                %
  \begin{equation}
    \label{eq:EMmax2}
    \ti{\parameters}{n+1} = \argmax_{\parameters} Q(\parameters,
    \ti{\parameters}{n})
  \end{equation}
  where
  \begin{equation}
    \label{eq:Qdef2}
    Q(\parameters',\parameters) \equiv \EV_{P\left(\bS\given \y, \parameters \right)}
    \left( \log P(\y,\bS\given \parameters') \right)
  \end{equation}
\item Increment $n$.
\item If not converged, go to \ref{loop}.
\end{enumerate}
Step \ref{loop} does all of the work.  Note that if the unobserved
data $(\bS)$ is discrete, then the auxiliary function $(Q)$ is
$\EV_{P\left(\bS\given \y,\parameters \right)} \left( \log
  P(\y,\bS\given \parameters') \right) = \sum_\s P(\s\given \y,\parameters) \left(
  \log P(\y,\s\given _\parameters') \right)$.  Although Dempster Laird and
Rubin \cite{Dempster77} called the characterization of $P
\left(\bS\given \y,\parameters \right)$ the \emph{estimation step} and the
optimization of $Q(\parameters, \ti{\parameters}{n})$ over
$\parameters$ the \emph{maximization step}, the steps are now referred
to as \emph{expectation} and \emph{maximization}.

The advantages of the EM algorithm are that it is easy to implement
and it monotonically increases the likelihood.  These often outweigh
its slow convergence and the fact that it calculates neither the
second derivative of the likelihood function nor any other indication
of the reliability of the results it reports.  Proving monotonicity is
simple.  If the likelihood is bounded, convergence follows directly
from monotonicity, but convergence of the parameters does not follow.
Also, the likelihood might converge to a local maximum.  Papers by
Baum et al.\cite{Baum70}, Dempster, Laird, and Rubin\cite{Dempster77},
and Wu\cite{Wu83} analyze the issues.  In the next two subsections we
touch on some of the ideas and analyze an example.

\subsection{Monotonicity}

Denoting the log likelihood of the observed data given the model
$\parameters'$ as
\begin{equation*}
  L(\parameters') \equiv \log \left( P \left(\y\given \parameters' \right)\right)
\end{equation*}
and the cross entropy of the unobserved data with respect to a model
$\parameters'$ given a model $\parameters$ as
\begin{equation*}
  H(\parameters, \parameters') \equiv - \EV_{P
    \left(\bS\given \y,\parameters \right)} \left( \log P(\bS\given \y,\parameters')
  \right),
\end{equation*}
we can write the auxiliary function as
\begin{align*}
  Q(\parameters',\parameters) &\equiv \EV_{P \left(\bS\given \y ,\parameters\right)}
  \left( \log P(\bS,\y\given \parameters') \right)\\
  &= \EV_{P \left(\bS\given \y ,\parameters \right)} \left( \log
    P(\bS\given \y,\parameters') + \log \left( P(\y\given \parameters') \right)
  \right)\\
  &= L(\parameters') - H(\parameters, \parameters')
\end{align*}
or
\begin{equation}
  \label{eq:LQH}
   L(\parameters') = Q(\parameters',\parameters) + H(\parameters,
   \parameters').
\end{equation}
The fact that
\begin{equation}
  \label{eq:GibbsIE}
  H(\parameters, \parameters') \geq  H(\parameters, \parameters)
  \forall \parameters'
\end{equation}
with equality \emph{iff}
\begin{equation*}
  P(\s\given \y,\parameters') =  P(\s\given \y,\parameters) ~~ \forall \s
\end{equation*}
is called the \emph{\index*{Gibbs Inequality}}\footnote{While many
  statisticians attribute this inequality to Kullback and
  Leibler\cite{Kullback51}, it appears earlier in chapter {\em XI}
  {\em Theorem II} of Gibbs\cite{Gibbs}.}.  It is a consequence of
\index{Jensen's inequality} Jensen's inequality.

Given the model $\ti{\parameters}{n}$ after $n$ iterations of the EM
algorithm, we can write
\begin{equation}
  L(\ti{\parameters}{n}) = Q(\ti{\parameters}{n}, \ti{\parameters}{n}) +
  H(\ti{\parameters}{n}, \ti{\parameters}{n}).
\end{equation}
If for some other model $\parameters'$
\begin{equation}
  \label{eq:GEMcond}
  Q(\parameters',\ti{\parameters}{n}) >
  Q(\ti{\parameters}{n},\ti{\parameters}{n}),
\end{equation}
then the Gibbs inequality implies $H(\ti{\parameters}{n}, \parameters') \geq H(\ti{\parameters}{n},
\ti{\parameters}{n})$ and consequently, $L(\parameters') >
L(\ti{\parameters}{n})$.  Monotonicity of the log function further
implies
\begin{equation*}
  P \left(\y \given \parameters'\right) > P \left(\y\given \ti{\parameters}{n} \right) .
\end{equation*}
Since the EM algorithm requires the inequality in \eqref{eq:GEMcond},
for $\parameters' = \ti{\parameters}{n+1}$, the algorithm
monotonically increases the likelihood.

\subsection{Convergence}
%
Here we analyze how the Baum-Welch algorithm converges in a simple
example.  Some of the analysis applies to EM algorithms in general.
An EM algorithm operates on $\Theta$, a set of allowed parameter
vectors with a map $\EMmap:\Theta \mapsto \Theta$ that implements an
iteration of the algorithm, \ie,
\begin{equation*}
  \ti{\parameters}{n+1} = \EMmap (\ti{\parameters}{n}).
\end{equation*}
Wu\cite{Wu83} has considered convergence of EM algorithms generally
and observed that more than one value of $\parameters'$ may maximize
$Q(\parameters',\parameters)$.  Consequently he considered $\EMmap$ to
be a point to set map.  However, there are many model classes
--including all of the models that we describe in this book-- for
which one can write algorithms that calculate a unique
$\EMmap(\parameters)$.

If there is a bound\footnote{See Section~\ref{sec:regularization} for
  an example involving probability densities with no such bound.  For
  discrete $\y$, however the bound
  $ P \left(\y\given \parameters \right) \leq 1$ holds.}  $\bar P$ on
$ P \left(\y\given \parameters \right) $ with
\begin{equation*}
   P \left(\y\given \parameters \right) \leq \bar P ~ \forall \parameters \in \Theta
\end{equation*}
then the monotonicity of the sequence
$\left( P(\y\given \ti{\parameters}{1}), P(\y\given
  \ti{\parameters}{2}), P(\y\given \ti{\parameters}{3}), \ldots
\right)$ and the bounded convergence theorem ensures that the limit
$ P^* \equiv \lim_{n\rightarrow \infty} P \left(\y \given
  \ti{\parameters}{n} \right) $ exists.  The bounded convergence
theorem does not promise that
$P^* = \sup_{\parameters \in \Theta} P \left(\y \given \parameters
\right)$ or that it is even a local maximum, nor does it promise that
$\EMfixedPoint \equiv \lim_{n \rightarrow \infty} \ti{\parameters}{n}$
exists.  However, in the following example, iterations of $\EMmap$
converge to a local maximum\footnote{While Wu\cite{Wu83} provides an
  example of a trajectory that converges to a saddle point of the
  likelihood, we argue in the appendix that such trajectories are not
  generic.} of the likelihood, and we believe it provides good
intuition for behavior of the Baum-Welch algorithm.

Consider the model defined by the initial state probability
\begin{subequations}
  \label{eq:EM_convergence_example}
\begin{equation}
P_{\ti{S}{0}} =
\begin{bmatrix}
  \frac{1}{2}, & \frac{1}{2}
\end{bmatrix},  
\end{equation}
the state transition probability
\begin{equation}
\begin{array}{rr|cc}
  && \multicolumn{2}{c}{\ti{S}{t+1}} \\
  \multicolumn{2}{c|}{P(\ti{s}{t+1}|\ti{s}{t})} & 0 & 1 \\ \hline
  & 0 & .9 & .1  \\  && \vspace{-1 em} \\
  \ti{S}{t} & 1 & u & 1-u,
\end{array}
\end{equation}
and the observation probability
\begin{equation}
\begin{array}{rr|cc}
  && \multicolumn{2}{c}{\ti{Y}{t}} \\
  \multicolumn{2}{c|}{P(\ti{y}{t}|\ti{s}{t})} & 0 & 1 \\ \hline
  & 0 & .9 & .1  \\  && \vspace{-1 em} \\
  \ti{S}{t} & 1 & v & 1-v.
\end{array}
\end{equation}
\end{subequations}
A schematic of the model appears in Figure~\ref{fig:EMxfig}.  We set
\begin{equation}
  \label{eq:theta_true}
  \parameters_{\text{true}} \equiv \left(u_{\text{true}}, v_{\text{true}}
  \right) = \left(0.1, 0.2 \right)
\end{equation}
and simulated the model for 10,000 time steps to produce
$\ts{y}{0}{10,000}$.  Then starting with
\begin{equation}
  \label{eq:theta_initial}
  \parameters_{\text{initial}} \equiv \left(u_{\text{initial}}, v_{\text{initial}}
  \right) = \left(0.001, 0.01 \right)
\end{equation}
we ran fifty iterations of the Baum-Welch algorithm to calculate an
estimate of $\EMfixedPoint$.  During training we held all the model
parameters fixed at their true values except $u$ and $v$ .
%%%
%%% fig:EMxfig
%%%
\begin{figure}[htbp]
  \centering{ % The scope of these defs is limited by the enclosing {}
      \def\PointOne{$0.1$}
      \def\PointNine{$0.9$}
      \def\prbu{$u$}
      \def\prbOmu{$1-u$}
      \def\prbv{$v$}
      \def\prbOmv{$1-v$}
      \def\PointNine{$0.9$}
      \def\PointNine{$0.9$}
      \def\zero{$0$}
      \def\one{$1$}
      \input{EMxfig.pdf_t}
    }
  \caption[\comment{fig:EM }An HMM with only two parameters.]%
  {A schematic of the HMM defined by
    Equations~\eqref{eq:EM_convergence_example}.  We chose to have only two
    free parameters in the model, $u$ and $v$, so that we can
    illustrate the convergence of training with 2-d plots as we have
    in Figure~\ref{fig:EM}.}
  \label{fig:EMxfig}
\end{figure}
      
Figure~\ref{fig:EM} illustrates the trajectory in the parameter space,
$U \times V$, of the first eleven training iterations.  The
eigenvectors and eigenvalues that appear in the lower plot of that
figure are from a spectral decomposition of the second term in the
Taylor series expansion $\EMmap$ near $\EMfixedPoint$, ie,
\begin{equation*}
  \EMmap(\parameters) = \EMfixedPoint + D  (\parameters - \EMfixedPoint) +
             \text{Remainder.}
\end{equation*}
In Appendix \ref{chap:em_appendix} on page
\pageref{eq:information_em_derivative} we derive
\begin{equation*}
  D \equiv \left.\frac{\partial \EMmap(\parameters)}{\partial \parameters}
  \right|_\EMfixedPoint = \left[J_y + I_{S|y}  \right]^{-1} I_{S|y}.
\end{equation*}
To evaluate $D$ one can obtain the term $I_{S\given y}$ from
$P(s\given y, \parameters)$ which comes from the forward and backwards
algorithms.  On the other hand, we find
\begin{equation*}
  J_y \equiv -\frac{\partial^2}{\partial \parameters^2} \log \left( P(y \given
    \parameters \right),
\end{equation*}
which is called the \emph{observed information}, more difficult to
calculate.  For the figure, we approximated $J_y$ with numerically
expensive calculations.

Notice that the trajectory quickly lines up with the eigenvector of
$D$ whose eigenvalue is closest to 1 and then decays exponentially
towards $\EMfixedPoint$.  We believe that behavior is typical.  In the
appendix we also show that if all of the eigenvalues of $J_y$ are
positive\footnote{Positive eigenvalues implies that $\EMfixedPoint$ is
  a local maximum.} then $\EMmap$ is linearly stable, \ie,
$\left| \lambda \right| < 1$ for all eigenvalues of $D$.  If one could
cheaply estimate the spectral decomposition of $D$, one could speed up
the convergence of HMM training.

\begin{figure}[htbp]
  \centering{\resizebox{0.7\textwidth}{!}{\includegraphics{EM.pdf}}
  }
  \caption[\comment{fig:EM }An illustration of the EM algorithm.]%
  {An illustration of the EM algorithm converging to a maximum
    likelihood estimate $\EMfixedPoint$.  The data $\ts{y}{0}{10,000}$ come
    from simulating the HMM described in
    \eqref{eq:EM_convergence_example} and Figure~\ref{fig:EMxfig}.
    The first eleven estimates of $\parameters\equiv(u,v)$ in the training
    sequence appear as a blue trajectory in both plots.  In the upper
    plot level sets of the log-likelihood,
    $L(\parameters') = \log \left( P(\ts{y}{0}{10,000} \given
      \parameters')\right)$, and auxiliary function,
    $Q(\parameters',\ti{\parameters}{n})$ appear for every third value of $n$.
    Notice that every value of $\parameters'$ except $\ti{\parameters}{n}$ on
    the level set of $Q(\parameters',\ti{\parameters}{n})$ is inside of (and
    therefore has higher likelihood) the level set of $L$.  The
    eigenvectors depicted in the lower plot are from a spectral
    decomposition of the derivative
    $\left.  \frac{\partial \EMmap(\parameters)}{\partial\parameters}
    \right|_{\EMfixedPoint}$ given by
    Equation~\eqref{eq:information_em_derivative} on page
    \pageref{eq:information_em_derivative} of the appendix.  The
    lengths of the vectors are proportional to the logs of the
    eigenvalues.}
  \label{fig:EM}
\end{figure}

%%% Local Variables:
%%% TeX-master: "main"
%%% eval: (load-file "hmmkeys.el")
%%% mode: LaTeX
%%% End:
