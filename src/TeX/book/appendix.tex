In summary: Qualitatively $\EMmap$ acts like a gradient flow on the
log likelihood,
$\LogLikelihood(\parameters) \equiv\log \left( P(y \given \parameters
\right)$; convergence to fixed point of $\EMmap$ that is a local
maximum of the likelihood is generic and convergence to a saddle point
of the likelihood is not generic.

\subsubsection{Comments}
\label{sec:ConvergenceRate}

The analysis of this appendix shows that in a neighborhood of a local
maximum of the likelihood the largest eigenvalue,
$\lambda_{\text{max}}$, of $D$ is between 0 and 1.  In the limit of
large $n$ the convergence of the EM algorithm is linear, with an error
$\ti{\epsilon}{n} \equiv \ti{\parameters}{n} - \EMfixedPoint$
dominated by the largest eigenvalue, $\lambda_{\text{max}}$ of $D$,
with
\begin{align*}
  \ti{\epsilon}{n+1} \approx \lambda_{\text{max}} \ti{\epsilon}{n}
  &&\text{for large } n \text{ and }\\
  \lim_{n \rightarrow \infty} \frac{1}{n}\log(\ti{\epsilon}{n}) &= \log(\lambda_{\text{max}}).
\end{align*}

One generally uses the EM algorithm when maximizing the auxiliary
function $Q$ is much easier than estimating derivatives such as
$\delta(\parameters) \equiv \frac{\partial}{\partial \parameters} \log
\left( P(y \given \parameters )\right)$ or
$H(\parameters) \equiv \frac{\partial^2}{\partial \parameters^2} \log
\left( P(y \given \parameters \right)$.  If it is easy to estimate
$\delta$ and $H$, one should use the Newton scheme
\begin{equation*}
  \ti{\parameters}{n+1} = \ti{\parameters}{n} - \left(H(\ti{\parameters}{n})\right)^{-1} \delta(\ti{\parameters}{n})
\end{equation*}
which converges quadratically.  In
Equation~\eqref{eq:information_em_derivative} we have expressed $D$ in
terms of the second derivative $J_y\equiv -H$ and the term $I_{S|y}$
which would be even more difficult to calculate.  We do not suggest
calculating $J_y$, $I_{S|y}$, or $D$, because if you could do that,
you should use a Newton method.  The analysis here both assures that
generic trajectories of the EM algorithm only converge to local maxima
of the likelihood and in Equation~\eqref{eq:information_em_derivative}
gives one a sense of how the convergence rate depends on
characteristics of the likelihood and the unobserved data.
