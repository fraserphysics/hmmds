% Notes on Maximum A posteriori Probabilty (MAP) estimation.

% Copyright 2022 Andrew M. Fraser.
\documentclass[12pt]{article}
\usepackage{amsmath, amsfonts}


\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\INTEGER}{\field{Z}}
\newcommand{\REAL}{\field{R}}
\newcommand{\COMPLEX}{\field{C}}
\newcommand{\id}{\mathbb{I}}
\newcommand{\variance}[1]{\field{V}\left[ #1 \right]}
\newcommand{\normal}[2]{{\cal N}\left(#1,#2 \right)}

\title{Models of the Laser Data}
\author{Andrew M.\ Fraser}

\begin{document}
\maketitle

\section{Particle Filter for Linear Observation of Lorenz}
\label{sec:linear_observation}

Code is in hmmds/synthetic/filter/lorenz\_particle\_simulation.py

\subsection{Init}
\label{sec:init}

\begin{description}
\item[$\Sigma_\epsilon$ observation\_covariance:] 
\item[$O$ observation\_map:] 
\item[$I_y$ info\_y:] $O^T \Sigma_\epsilon^{-1} O$
\item[$\Sigma_\eta$ state\_covariance:] 
\item[$\Sigma_i$ importance\_covariance:] $\Sigma_i = \left(
    \Sigma_\eta^{-1} + I_y \right)^{-1}$
\item[$G_i$ $importance\_gain$:] $G_i = \Sigma_i \cdot O^T \cdot
  \Sigma_\epsilon^{-1} $
\item[$\Phi: X \mapsto X$ Integrate Lorenz for dt:] 
\item[$p(x_{t+1}|x_t)$ transition:] 
\item[$p(y_t|x_t)$ observation:] 
\item[$q(y_{t+1},x_t)$ importance:] $q(x[t+1]|y[t+1], x[t]) =
  p(x[t+1]|y[t+1], x[t])$.  The importance method draws $x[t+1]$
  from the $q$ distribution and returns both $x[t+1]$ and the value
  of $q$.
\end{description}

\subsection{Importance Method}
\label{sec:importance}

The argument $observation\_map$ affects the following attributes of
self: $observation\_map$, $importance\_distribution$,
$importance\_gain$, and $importance\_covariance$.  In turn those
attributes are used in the following methods: $observation$ and
$importance$.  To do particle filtering on the laser data, I need a
new $importance$ method that accounts for the nonlinear
$observation\_map$.

Let me start by describing the importance method for the linear
observation.
\begin{description}
\item[$\mu_{\text{forecast}}$:]  $\phi(x[t])$
\item[$\Delta_{\text{forecast}}$:] $y[t+1] - O \cdot \mu_{\text{forecast}}$
\item[$\mu_{\text{update}}$:]  $\mu_{\text{forecast}} + G_i \cdot
  \Delta_{\text{forecast}}$
\item[$\eta_{t+1}$:] Draw from $importance\_distribution$
\item[$x_{t+1}$:] $\mu_{\text{update}} + \eta[t+1]$
\end{description}
The method returns $x[t+1]$ and $p(\eta[t+1])$ using a distribution
$\normal{\mu_i}{\Sigma_i}$.  Key to calculating $\mu_i$ is the
\emph{importance gain}, $G_i$.

Here are calculations of $\Sigma_i$, $\mu_i$ and $G_i$.  Note for
\begin{equation*}
  p(X[t+1]|y[t+1], x[t]) \propto p(X[t+1]|x[t]) \cdot p(y[t+1]|X[t+1])
\end{equation*}
I can ignore normalization, and focus only on the exponent.
Letting $E$ be -2 times the exponent, I write
\begin{align*}
  E = & \left( X[t+1] - \Phi(x[t]) \right)^T \Sigma_\eta^{-1} \left(
    X[t+1] - \Phi(x[t]) \right) \\ +& \left( y[t+1] - O X[t+1] \right)^T
  \Sigma_\epsilon^{-1} \left( y[t+1] - O X[t+1] \right)
\end{align*}
Using the abbreviations
\begin{align*}
  X &= X[t+1] \\
  \Phi &= \Phi(x[t]) \\
  y &= y[t+1] \\
  \mu_i &= \mu \\
  \Sigma_i &= \Sigma,
\end{align*}
I write
\begin{align}
  \label{eq:AbbrevE}
  E &= \left( X-\Phi \right)^T \Sigma_\eta^{-1} \left( X-\Phi \right) +
  \left( y - OX \right)^T \Sigma_\epsilon^{-1} \left( y - OX \right)
  \\ \nonumber
  &= X^T \Sigma_\eta^{-1} X -2 \Phi^T \Sigma_\eta^{-1} X + \Phi^T
    \Sigma_\eta^{-1} \Phi \\ \nonumber
    &\quad + y^T \Sigma_\eta^{-1} y - 2 y^T \Sigma_\eta^{-1} OX +
      X^TO^T \Sigma_\eta^{-1} OX \\  \nonumber
  &= X^T\left(\Sigma_\eta^{-1} + O^T \Sigma_\epsilon O \right) X - 2
    \left( \Phi^T \Sigma_\eta^{-1} + y^T \Sigma_\epsilon^{-1} O
    \right) X + R
\end{align}
Thus $\Sigma = \left(\Sigma_\eta^{-1} + O^T \Sigma_\epsilon O
\right)^{-1}$ and since
\begin{align*}
  E &= (X-\mu)^T \Sigma^{-1}(X-\mu) \\
    &= X^T \Sigma^{-1} X - 2\mu^T\Sigma^{-1} X + R \\
    &= X^T \Sigma^{-1} X - 2X^T \Sigma^{-1} \mu + R
\end{align*}
I can write
\begin{align*}
  \Sigma^{-1} \mu &= \Sigma_\eta^{-1} \Phi + O^T \Sigma_\epsilon^{-1}
                    y \\
               \mu &= \Sigma\Sigma_\eta^{-1} \Phi + \Sigma O^T
                     \Sigma_\epsilon^{-1} y,
\end{align*}
and since (by multiplying both sides by $\Sigma^{-1}$ one can verify)
\begin{equation*}
  \Sigma \Sigma_\eta^{-1} = \id - \Sigma O^T \Sigma_\eta^{-1} O,
\end{equation*}
I can write
\begin{align*}
  \mu &= \left( \id - \Sigma O^T \Sigma_\eta^{-1} O \right) \Phi +
        \Sigma O^T \Sigma_\epsilon^{-1} y \\
      &= \Phi + \Sigma O^T \Sigma_\epsilon^{-1} (y - O\Phi)
\end{align*}
In summary:
\begin{subequations}
  \label{eq:LinearImportance}
  \begin{align}
    \Sigma_i &= \left( \Sigma_\eta^{-1} + O^T \Sigma_\epsilon^{-1} O
               \right)^{-1} \\
    G_i &= \Sigma_i O^T \Sigma_\epsilon^{-1} \\
    \mu_i &= \Phi(x[t]) + G_i \left(y[t+1] - O \Phi(x[t]) \right)
  \end{align}
\end{subequations}

\section{Particle Filter for Nonlinear Observation of Lorenz with
  Gaussian Observation Noise}
\label{sec:nonlinear}

Here the observation map is a nonlinear function $\psi$.  I want to
use its first order approximation
\begin{align*}
  \psi(X) &\approx \psi(\Phi) + \psi' (X-\Phi) \quad \text{where
            $\psi'$ is evaluated at $\Phi$}\\
  &= \psi(\Phi) - \psi'\Phi + \psi' X
\end{align*}
in a derivation that mimics \eqref{eq:AbbrevE}.  I replace the term
$(y-OX)$ in \eqref{eq:AbbrevE} with $(z - \psi'X)$ where
\begin{equation*}
  z \equiv y - \psi(\Phi) + \psi' \Phi.
\end{equation*}
The nonlinear version of \eqref{eq:LinearImportance} is
\begin{subequations}
  \begin{align}
    \Sigma_i &= \left( \Sigma_\eta^{-1} + \psi'^T \Sigma_\epsilon^{-1} \psi'
               \right)^{-1} \\
    G_i &= \Sigma_i \psi'^T \Sigma_\epsilon^{-1} \\ \nonumber
    \mu_i &= \Phi + G_i \left(z - \psi' \Phi) \right) \\ \nonumber
             &= \Phi + G_i \left(y - \psi(\Phi) ) \right) \\
             &= \Phi(x[t]) + G_i \left(y[t+1] - \psi(\Phi(x[t]))
               \right)
  \end{align}
\end{subequations}

The code in particle.py uses the above formulas and parameters in
optimize.py.

\begin{tabular}[t]{l|l|r}
  code&comment&log likelihood \\ \hline
  ekf.py & & -6422.0 \\
  particle.py & filtered data a little rough. 18 minutes &
                                                               -6762.4 \\
  particle.py & $\Sigma_\eta /= 100$, filtered data is smooth &
                                                               -12912.5 \\
\end{tabular}

\section{Discrete Observations}
\label{sec:DiscreteObservations}

\section{Optimization}
\label{sec:Optimization}

\section{To Do}
\label{sec:ToDo}

\begin{enumerate}
\item Automate parameter optimization and document it here
\item Write code for particle filter with discrete observations and document
\item Optimize parameters for particle filter
\item Make plot for Introduction
\end{enumerate}
Notes on pymp:
\begin{verbatim}
 5257  git clone https://github.com/classner/pymp.git
 5258  cd pymp/
 5264  python setup.py install --prefix=./
 5281  export PYTHONPATH=/mnt/precious/home/andy_nix/projects/dshmm:/mnt/precious/home/andy_nix/projects/proj_hmm/src:/mnt/precious/home/andy_nix/projects/dshmm:/mnt/precious/home/andy_nix/projects/dshmm/pymp:

Without pymp.  Commit.
[nix-shell:~/projects/dshmm/hmmds/applications/laser]$ make
time python particle.py --n_times 50 powell250.parameters test_particle
log_likelihood=-74.8157538232742

real	0m31.962s
user	1m19.362s
sys	0m0.233s

\end{verbatim}
\end{document}
